[
  {
    "objectID": "posts/HW2/index.html",
    "href": "posts/HW2/index.html",
    "title": "Scraping TMDB with Scrapy",
    "section": "",
    "text": "For this blog post, we will dive into web scraping. We will be looking into the movie database (TMDB) as our website which we will scrape data from. The goal of this article will be to look at a specific TV show, find all of its actors, and record their whole filmography (as actors). After reading through this article, you will be able to:"
  },
  {
    "objectID": "posts/HW2/index.html#step1",
    "href": "posts/HW2/index.html#step1",
    "title": "Scraping TMDB with Scrapy",
    "section": "Extract elements from a web page",
    "text": "Extract elements from a web page\nThis is practice to figure out what CSS elements are needed to be extracted. This is done manually so that you get a good sense of how to access specific elements. In this example, we will be looking at the web page corresponding to Community.\nIn general, the framework of extracting elements is as follows: - find the element you want to extract - right-click on the element, and select “Inspect”. - note the HTML tags that uniquely identify that element.\n\nNavigate to “Full Cast & Crew”\nFor this project, we want to view the “Full Cast & Crew” page of the TV page. We will find that in the middle-left of the page, and we can see that the HTML tags wrapped around it are <section class=\"panel top_billed\">, <p> and <a> tags. We can see the URL we want to go to reflects a relative path that adds to the current web page (i.e. /cast).\n\nUnderlying HTML Code: \nWe will now go to this page.\n\n\nNavigate to an Actor’s Page\nSince we want to find the filmography of every actor, we need to navigate to each individual actor’s page. For this example, we will look at navigating to Joel McHale’s (plays Jeff Winger in Community) page.\nThe element we want is McHale’s headshot, which provides a URL to go to his page. We inspect this element, and see that his headshot is within a <ol class=\"people credits\">, <li>, <a> tag. We want the URL found in the <a> tag, in the attribute href.\n\nUnderlying HTML Code: \nWe can now navigate to his page.\n\n\nExtract all the works they have acted in\nNow, we are able to look at McHale’s work in the Acting section. We will look at how to extract his name from the page, as well as “California King” from the filmography.\nFirst, we will extract his name from the big title on the top of the page. We can see that the element is wrapped in a <div class=\"title\">, and <a> tag.\n\nUnderlying HTML Code: \nNow, let us look at “California King”. We can see that it is wrapped in a <table class=\"card credits\">, <a class=\"tooltip\">, and <bdi> tag.\n\nUnderlying HTML Code: \nWith this information, we can now begin to automate this process with scrapy."
  },
  {
    "objectID": "posts/HW2/index.html#step2",
    "href": "posts/HW2/index.html#step2",
    "title": "Scraping TMDB with Scrapy",
    "section": "Create A Spider",
    "text": "Create A Spider\nThis section will look at the creation of a spider. There are a couple of steps that you need to take.\n\nCreate a new repository\nEnsure scrapy is in your environment\nStart a scrapy project in the repository\n\n\nCreate a new repository\nThe first step is to create a new GitHub repository. This is done through a GitHub account. This repository will house your spider, and can be version controlled to account for different changes. Sync it up with your corresponding GitHub Desktop account, and clone the repository to your local device with this tutorial.\nNow that the repository is local, we can move on to the next step.\n\n\nEnsure scrapy is in your environment\nThe easiest way to do this is to open up Anaconda-Navigator, and to look at the tab “Environments”. Find the environment that corresponds to the one you will run the spider on. Search for scrapy on the installed packages. If it doesn’t show up, find it on the uninstalled packages and install it.\n\n\nStart a scrapy project in the repository\nSince you have scrapy installed and a local repository, navigate to the local repository via Terminal, and type the following:\nconda activate <myenv>\nscrapy startproject TMDB_scraper\ncd TMDB_scraper\nThis will automatically create a spider with the necessary python files for it to run. In particular, we will use settings.py. Go into the spiders folder and also create a tmdb_spider.py file and write this as the base.\n\n# to run \n# scrapy crawl tmdb_spider -o movies.csv\n\nimport scrapy\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n    \n    start_urls = ['https://www.themoviedb.org/tv/18347-community']"
  },
  {
    "objectID": "posts/HW2/index.html#step3",
    "href": "posts/HW2/index.html#step3",
    "title": "Scraping TMDB with Scrapy",
    "section": "Automate extraction through the spider",
    "text": "Automate extraction through the spider\nWe will now automate extraction through the spider. This is where we will need to recall the information we gained earlier (see above), as well as have knowledge of CSS selectors. Find out more about CSS selectors here and here. As you experiment with how to extract this via code, it is helpful to use scrapy shell instead of using trial and error and running .py files.\n\nScrapy Shell\nHere is a small example of using scrapy shell for experimentation. Navigate to the repository, open up a terminal, and run scrapy shell https://www.themoviedb.org/tv/18347-community(NOTE: You may run into errors such as 403, please look to here for guidance on how to resolve that; you want to see numbers of the form 200).\nHere is what happens when you run scrapy shell https://www.themoviedb.org/tv/18347-community, which is a request to access the main page for Community.\n\nYou can see that there is are two lines of GET (200) ..., which signifies that the request went through. I then sent a request to find the URL for the “Full Cast & Crew” page, by subsetting via the tags it is wrapped in:\nresponse.css(\"section.top_billed p.new_button a\").attrib['href']\nThis can be done to experiment with other subsetting to ensure you are getting the right elements.\n\n\nCreating Functions to Scrape TMDB\nNow, we are able to create a web scraper, complete with functions that will do it for us at one command. There are three main functions necessary for us to scrape the filmography of actors from Community. We need a function to parse to main TV page, one to parse the list of actors, and one to parse the actor’s filmography. All of these functions should go into the tmdb_spider.py file under the TmdbSpider class. I will go through each one.\n\nRepository\nHere is the repository that houses all the data and code for this scraper.\n\n\nParsing main TV page\n\ndef parse(self, response):\n        '''\n        Parses the TMDB cast and crew website.\n        @ input:\n        - self: TmdbSpider\n        - response: the call to start_urls[0]\n        @ output:\n        - yields a request to navigate to the page with actors.\n        '''\n        cast = self.start_urls[0] + '/cast' # hardcode the cast page\n        \n        # go to cast & crew page, run parse_full_credits\n        yield scrapy.Request(cast, callback = self.parse_full_credits)\n\nThis function gets the starting URL, which is linked to the TMDB Community page. Since we recognize that the “Full Cast & Crew” can be found by just adding /cast to the end of the URL, we just hardcode that in, and send a request to go to that URL, and to run parse_full_credits.\n\n\nParsing the acting credits page\n\ndef parse_full_credits(self, response):\n        '''\n        for each actor, goes to their respective acting profile page on TMDB.\n        @ input:\n        - self: TmdbSpider\n        - response: the call to the \"Full Cast & Crew\" page\n        @ output:\n        - yields a request to navigate to the profile page of each actor.\n        '''\n        \n        # for each page redirection on the cast photos\n        for page in response.css('ol.people.credits:not(.crew) li a'):\n            actor_page = page.attrib['href'] # obtain the hyperlink\n            actor_page = 'https://www.themoviedb.org' + actor_page # append to main url\n            \n            # go to the actor's page, run parse_actor_page\n            yield scrapy.Request(actor_page, callback = self.parse_actor_page)\n\nThis function assumes that we are on the page with all the actors in Community. We now need to navigate to every actor’s home page. As mentioned earlier, we know that the link is contained within the HTML tags of <ol class=\"people credits\">, <li>, and <a>. However, that alone is not enough to get ONLY the actors; it includes the crew members. We don’t want that, so looking further into the HTML code, we see that the crew members has a special designation in their tag <ol class=\"people credits crew\">. We can use this to our advantage to omit any <ol> tags with class “crew” with the :not() CSS selector. Now, we get that for every element within the actors section, obtain the hyperlink for their page, and submit a request to go to the actor’s profile for every actor in Community, and run parse_actor_page.\n\n\nParsing actor’s profile page\n\ndef parse_actor_page(self, response):\n        '''\n        obtains the films of the actor.\n        @ input:\n        - self: TmdbSpider\n        - response: the call to the actor's page\n        @ output:\n        - yields a dictionary with actor name and movie.\n        '''\n        # obtain the actor name\n        actor_name = response.css('div.title a::text').get()\n        \n        # for each of the links in the acting section of his or her page\n        for acting_gig in response.css('h3.zero + table.card.credits a.tooltip bdi::text'):\n            title = acting_gig.get() # obtain the right URL\n            \n            yield {'actor': actor_name, 'movie_or_TV_name': title} \n            # yield a dictionary with actor and title of movie they were in.\n\nThis function assumes that you are on the actor’s page. We first need to get the actor’s name for our dictionary, which is conveniently placed in bold font at the top of the page in a <div class=\"title\"> and <a> tag. The harder part is getting the movies/TV shows that the actor has acted in. In this case, it is found underneath <h3 class=\"zero\">. The selectors to the right of the + operator denote that you only want to find elements within h3.zero. You go within the <table class=\"card credits\"> and <a class=\"tooltip\"> and <bdi> tags to get the text within these subsets. That will give you the title. For every film, we can yield a dictionary linking a person with a specific film.\nWith these three functions, we are able to run the spider and have it crawl the TMDB website for Community websites. This can apply to any other start URL for other movies or TV shows."
  },
  {
    "objectID": "posts/HW2/index.html#step4",
    "href": "posts/HW2/index.html#step4",
    "title": "Scraping TMDB with Scrapy",
    "section": "Figure out how to avoid blockers",
    "text": "Figure out how to avoid blockers\nThis is something that will be common as you scrape the web more. Websites will have methods that will determine if there is a bot or a program scraping data. If they detect suspicious behavior of this sort, they will block the request to obtain data, which will be displayed in the form of 40_ errors. This is a hinderance to data collection, but there are workarounds for it.\nFor this example with the default settings, TMDB will block requests to access their data, creating a 403 error. My workaround was based off of this article: I changed three lines in my settings.py file.\nBy default, your scrapy user agent will identify itself as a scraper when it goes to scrape websites. However, this is a clear indicator for websites to block the request. I changed the user agent by assigning a common user agent in my settings.py file.\n\n## settings.py\n\nUSER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.79 Safari/537.36 Edge/14.14393'\n\nAnother way that websites can discern a suspicious request is the speed of download requests. An automated bot that knows where to look would be faster than any human interaction with the interface. Therefore, it is important to set a good delay of download to not raise suspicions.\n\n## settings.py\n\n# Configure a delay for requests for the same website (default: 0)\n# See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay\n# See also autothrottle settings and docs\nDOWNLOAD_DELAY = 5\n\nThe last thing I did was disable cookies, because some websites use cookies to spot bot behavior.\n\n## settings.py\n\n# Disable cookies (enabled by default)\nCOOKIES_ENABLED = False\n\nHowever, these changes may not work for everyone. I would advise you to check out these other links (link1, link2, link3) for more help."
  },
  {
    "objectID": "posts/HW2/index.html#bonus-visualizing-scraped-data",
    "href": "posts/HW2/index.html#bonus-visualizing-scraped-data",
    "title": "Scraping TMDB with Scrapy",
    "section": "Bonus: Visualizing Scraped Data",
    "text": "Bonus: Visualizing Scraped Data\nWith this scraper, we are able to save our findings in a .csv file with the actor and respective movies they acted in. This is done through the terminal, running the command\nscrapy crawl tmdb_spider -O movies.csv\nThis command is telling the name of our spider tmdb_spider to scrape the web, and to overwrite output (-O) on a relative path to a file movies.csv. Replace -O with -o if you want to append the output to the current file instead. Now, you should be able to have the data accessible in movies.csv for you to use. We can use this data to create visualizations for the most common movies/TV shows that these actors have also played in.\n\nimport pandas as pd\nimport numpy as np\n\nmovies = pd.read_csv(\"./movies.csv\")\n\n\nmovies\n\n\n\n\n\n  \n    \n      \n      actor\n      movie_or_TV_name\n    \n  \n  \n    \n      0\n      Joel McHale\n      California King\n    \n    \n      1\n      Joel McHale\n      Mortal Kombat Legends: Cage Match\n    \n    \n      2\n      Joel McHale\n      Untitled Community Movie\n    \n    \n      3\n      Joel McHale\n      Parachute\n    \n    \n      4\n      Joel McHale\n      Animal Control\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      16948\n      Gillian Jacobs\n      American Dad!\n    \n    \n      16949\n      Gillian Jacobs\n      Building Girl\n    \n    \n      16950\n      Gillian Jacobs\n      The Venture Bros.\n    \n    \n      16951\n      Gillian Jacobs\n      Law & Order: Criminal Intent\n    \n    \n      16952\n      Gillian Jacobs\n      Aqua Teen Hunger Force\n    \n  \n\n16953 rows × 2 columns\n\n\n\n\n# get the count of actors in each TV/movie\nshared_movies = movies.groupby('movie_or_TV_name').agg(len).sort_values('actor', ascending = False).reset_index()\n\n\nt20_shared_movies = shared_movies[1:21] # gets the first 20 films, besides Community (#1)\n\n\nimport plotly.express as px\n\nfig = px.bar(t20_shared_movies,\n            x = 'movie_or_TV_name',\n            y = 'actor',\n            labels = {'movie_or_TV_name': 'Movie/TV Show',\n                     'actor': 'Number of Shared Actors'},\n            title = \"Top 20 Shows with Shared Actors with Community Cast\",\n            height = 700,\n            text_auto = '.2s')\nfig.show(renderer = 'notebook')\n#fig.show()\n\n\n                                                \n\n\nThis ranking of shows with shared actors is interesting. Coming into this post, I expected that there would be at least some Marvel movie, given that Community is directed by the Russo Brothers, and a majority of the cast has appeared on some of their movies. Instead, we are given many different TV shows. This makes sense given that people have the potential to make guest appearances to most of these shows. Likewise, Community itself is a show with many guest appearances due to their many homages to popular culture and movie tropes. A future direction would be to select only the actors who appear in a “majority” of episodes, and to only include recurring characters (not one episode guest appearances), to see if this list would change. However, we can see from this that the whole cast of Community has a wide array of actors with popular TV shows on their filmography."
  },
  {
    "objectID": "posts/HW2/index.html#conclusion",
    "href": "posts/HW2/index.html#conclusion",
    "title": "Scraping TMDB with Scrapy",
    "section": "Conclusion",
    "text": "Conclusion\nNow, you should be able to get started with web scraping! Take your time in learning CSS selectors, and finding the right tags to be your parameters. Be patient if it takes a while to fetch your requests, especially if websites block it. There is a workaround for most websites. Test it out for yourself and see the plethora of data that becomes available to you!"
  },
  {
    "objectID": "posts/HW5/index.html",
    "href": "posts/HW5/index.html",
    "title": "9/10 Journalisms Hate This Model",
    "section": "",
    "text": "The purpose of this blog post is to dive deeper into the functionality of TensorFlow in building neural networks. This time, we will be looking into classifying articles as fake news depending on the article text, the title, or both.\n\n\nLet us first get the data necessary for the project. This dataset is originally from a research article:\n\nAhmed H, Traore I, Saad S. (2017) “Detection of Online Fake News Using N-Gram Analysis and Machine Learning Techniques. In: Traore I., Woungang I., Awad A. (eds) Intelligent, Secure, and Dependable Systems in Distributed and Cloud Environments. ISDDC 2017. Lecture Notes in Computer Science, vol 10618. Springer, Cham (pp. 127-138).\n\nThey have publicized their data on Kaggle.\nAnother person (Phil Chodrow) has taken the liberty to process the data and host the data set in a URL for us to readily access. Below is the code to load in the necessary data, along with all the necessary packages we need for this project.\n\nimport pandas as pd\nimport numpy as np\n\n\n# load data\ntrain_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true\"\nnews_df = pd.read_csv(train_url)\nnews_df.columns = ['id', 'title', 'text', 'fake']\n\n\nnews_df.head()\n\n\n\n\n\n  \n    \n      \n      id\n      title\n      text\n      fake\n    \n  \n  \n    \n      0\n      17366\n      Merkel: Strong result for Austria's FPO 'big c...\n      German Chancellor Angela Merkel said on Monday...\n      0\n    \n    \n      1\n      5634\n      Trump says Pence will lead voter fraud panel\n      WEST PALM BEACH, Fla.President Donald Trump sa...\n      0\n    \n    \n      2\n      17487\n      JUST IN: SUSPECTED LEAKER and “Close Confidant...\n      On December 5, 2017, Circa s Sara Carter warne...\n      1\n    \n    \n      3\n      12217\n      Thyssenkrupp has offered help to Argentina ove...\n      Germany s Thyssenkrupp, has offered assistance...\n      0\n    \n    \n      4\n      5535\n      Trump say appeals court decision on travel ban...\n      President Donald Trump on Thursday called the ...\n      0\n    \n  \n\n\n\n\nThe data frame contains 4 columns: id, title, text, and fake. Here is what each variable represents:\n\nid (int): A unique identifier for the article\ntitle (str): Title of the article\ntext (str): full body text of the article\nfake (bool): whether or not the article is fake news (true label)\n\n\n\n\nEven though this dataset is cleaned up well, there are some more preprocessing steps that are needed before we can model the data. Since we are analyzing text data, we need to do two things:\n\nRemove stopwords from the article text and title. A stopword is classified as any word that is common and uninformative for analysis, such as “and”, “but”, “or”, “the”.\nTransform the dataset into one that can be read by TensorFlow, which is done through tf.data.Dataset.\n\nWe will create a function to automate these two processes.\n\nimport nltk\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nnltk.download('stopwords')\n\nfrom nltk.corpus import stopwords\n\ndef make_dataset(df, text_cols, response_var):\n  '''\n  creates a cleaned tensorflow dataset from a raw dataset.\n  @ inputs:\n  - df (pd.DataFrame): dataFrame containing columns of text data\n  - text_cols (str): column name(s) that represents text\n  - response_var (str): column name that represents the response variable\n  @ outputs:\n  - tfdf (tf.data.Dataset): output dataset with two inputs ((title, text)) and one output (fake).\n  '''\n  # gets stopwords from nltk.corpus library\n  stop = stopwords.words('english')\n\n  new_textcols = []\n  for col in text_cols:\n    new_colname = col + '_wo_stopwords'\n    new_textcols.append(new_colname) # new column name\n    \n    # removes stopwords from text, joins it all together again.\n    df[new_colname] = df[col].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n  \n  # creates a tensor dataframe from the processed text columns as input, and response var as output\n  tfdf = tf.data.Dataset.from_tensor_slices((\n      {\n          \"title\": df[[new_textcols[0]]],\n          \"text\": df[[new_textcols[1]]]\n      }, {\n          \"fake\": df[['fake']]\n      }\n      ))\n  \n  # speeds up performance\n  tfdf = tfdf.batch(100)\n  return tfdf\n\n2023-03-10 22:39:27.631067: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/joshuali/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\n\nnews_data = make_dataset(news_df, ['title', 'text'], 'fake')\n\n2023-03-10 22:39:55.202223: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\n\n\nNow that we have the data in a compatible format, we can now take this training data, and split 20% of it for validation purposes in hopes to help improve performance on the unseen data.\n\n# shuffles the data to ensure no ordering happens with splitting training and validation\nnews_data = news_data.shuffle(buffer_size = len(news_data))\n\n\n# 80% training data, 20% validation data\n\ntrain_size = int(0.8*len(news_data)) \nval_size = int(0.2*len(news_data))\n\ntrain = news_data.take(train_size) # data[:train_size]\nval = news_data.skip(train_size).take(val_size) # data[train_size : train_size + val_size]\n\n\n\n\nNow, let us look at the base rate that we would like to hit in order to determine whether our model performance is poor or excellent. If our model just guesses one label (e.g. ‘fake’), the proportion will be the base rate. We want to do better than this.\n\nfor x, y in train.take(1):\n  print(y['fake'].numpy().sum())\n  print(len(y['fake'].numpy()))\n\n66\n100\n\n\n\nfakes = 0\ntotal = 0\n\n# for inputs (x) and output (y) in train\nfor x, y in train:\n  # get number of fakes and total\n  fakes += y['fake'].numpy().sum()\n  total += len(y['fake'].numpy())\n# get baseline rate\nprint((fakes/total))\nprint(total)\n\n0.5248203242520475\n17949\n\n\nWe can see that the base rate is around 52%. We need to get above this percent accuracy so that our model performs better than just guessing ‘fake’ all the time.\n\n\n\nIn addition to this, we also need to make sure that these words are able to be rendered into the model. The neural network model cannot evaluate strings, so we have to vectorize each word to map to an integer. Here is one of the examples that you can use for the data.\n\nimport re\nimport string\n#preparing a text vectorization layer for tf model\nsize_vocabulary = 2000\n\ndef standardization(input_data): # word2vec\n    lowercase = tf.strings.lower(input_data)\n    no_punctuation = tf.strings.regex_replace(lowercase,\n                                  '[%s]' % re.escape(string.punctuation),'')\n    return no_punctuation \n\n# vectorize layer\ntitle_vectorize_layer = layers.TextVectorization(\n    standardize=standardization,\n    max_tokens=size_vocabulary, # only consider this many words\n    output_mode='int',\n    output_sequence_length=500) \n\n# adapts to the title words\ntitle_vectorize_layer.adapt(train.map(lambda x, y: x['title']))\n\n\n\n\n\nWe will use this processed informations and layers to help create models to classify the integrity of the news article. We will create three different models:\n\nUsing only the article title as an input.\nUsing only the article text as an input.\nUsing both the the article title and text as input.\n\n\n\n\n# base input that takes in Article Title\ntitle_input = keras.Input(\n    shape=(1,),\n    name = \"title\", # same name as the dictionary key in the dataset\n    dtype = \"string\"\n)\n\n\ntitle_features = title_vectorize_layer(title_input) # apply this \"function TextVectorization layer\" to lyrics_input\ntitle_features = layers.Embedding(size_vocabulary, output_dim = 3, name=\"title_embedding\")(title_features) # embeds words\ntitle_features = layers.Dropout(0.2)(title_features) # prevent overfit\ntitle_features = layers.GlobalAveragePooling1D()(title_features)\ntitle_features = layers.Dropout(0.2)(title_features) # prevent overfit\ntitle_features = layers.Dense(32, activation='relu')(title_features) # merge\n\n# x -> f(x) -> f1(f(x)) -> ... -> fn(...f(x))\n\n\noutput = layers.Dense(1, name=\"fake\")(title_features) # output of 1 since binary\n\n\n# title model\ntitle_model = keras.Model(\n    inputs = [title_input],\n    outputs = output\n)\n\n\ntitle_model.summary()\n\nModel: \"model\"\n\n\n_________________________________________________________________\n\n\n Layer (type)                Output Shape              Param #   \n\n\n=================================================================\n\n\n title (InputLayer)          [(None, 1)]               0         \n\n\n                                                                 \n\n\n text_vectorization (TextVec  (None, 500)              0         \n\n\n torization)                                                     \n\n\n                                                                 \n\n\n title_embedding (Embedding)  (None, 500, 3)           6000      \n\n\n                                                                 \n\n\n dropout (Dropout)           (None, 500, 3)            0         \n\n\n                                                                 \n\n\n global_average_pooling1d (G  (None, 3)                0         \n\n\n lobalAveragePooling1D)                                          \n\n\n                                                                 \n\n\n dropout_1 (Dropout)         (None, 3)                 0         \n\n\n                                                                 \n\n\n dense (Dense)               (None, 32)                128       \n\n\n                                                                 \n\n\n fake (Dense)                (None, 1)                 33        \n\n\n                                                                 \n\n\n=================================================================\n\n\nTotal params: 6,161\n\n\nTrainable params: 6,161\n\n\nNon-trainable params: 0\n\n\n_________________________________________________________________\n\n\n\nkeras.utils.plot_model(title_model) # model architecture\n\nYou must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n\n\n\n\n\nTitle Model Architecture\n\n\n\ntitle_model.compile(optimizer=\"adam\",\n              loss = keras.losses.BinaryCrossentropy(from_logits=True), # binary classification\n              metrics=[\"accuracy\"])\n\n#| id: hSp-ID4re9BV\n#| colab: {base_uri: 'https://localhost:8080/'}\n#| outputId: e3a4d60b-a850-4ca8-fa73-8a0fadc95b8d\ntitle_history = title_model.fit(train, \n                    validation_data=val,\n                    epochs = 50, \n                    verbose = False)\n#| id: fBdT6LJze-Ww\n#| colab: {base_uri: 'https://localhost:8080/', height: 609}\n#| outputId: fdb6cc9c-5420-4d03-e8b1-cd3c00400366\nfrom matplotlib import pyplot as plt\ndef plot_val(history, threshold, figsize = (10, 10), title = 'Model Performance', fontsize = 16):\n  '''\n  plots the training and validation accuracy of a model\n  @ inputs:\n  - history: gets the log of all metrics recorded by epoch in the neural net model\n  - threshold (float): the accuracy to surpass\n  - figsize (tuple): len 2 tuple to denote dimensions of plot\n  - title (str): Title of plot\n  - fontsize (float): font size of title\n  @ outputs:\n  - shows plot of training and validation accuracy by epoch\n  '''\n  fig, ax = plt.subplots(1, figsize = figsize)\n\n  # plots accuracies as lines\n  ax.plot(history.history[\"accuracy\"], label = 'Training Accuracy')\n  ax.plot(history.history[\"val_accuracy\"], label = 'Validation Accuracy')\n\n  # marks the threshold accuracy\n  ax.axhline(y = threshold, c = 'red', ls = '--')\n\n  # Customization\n  ax.legend()\n  ax.set_title(title, fontsize = fontsize)\n  fig.show()\n\nplot_val(title_history, 0.97, title = 'Model Performance with Title Input')\n\n\n\nPlot1\n\n\nWith the titles as the only input, the model performs extremely well in classifying the news article as fake or real news. The red line denotes the 97% accuracy line, which both the training and validation accuracy surpass over a number of epochs. There is no evidence of overfitting, as the validation accuracy consistently surpasses that of the training accuracy by a small margin.\n\n\n\nNow, we are going to look at a model that only takes text as the input. The process will be very similar as to the first model, but instead of inputting the title variable, we will use the text variable.\n\n# base text input\ntext_input = keras.Input(\n    shape=(1,),\n    name = \"text\", # same name as the dictionary key in the dataset\n    dtype = \"string\"\n)\n\n\ntext_vectorize_layer = layers.TextVectorization(\n    standardize=standardization,\n    max_tokens=size_vocabulary, # only consider this many words\n    output_mode='int',\n    output_sequence_length=500) \n\n# adapts vectorize layer to text words\ntext_vectorize_layer.adapt(train.map(lambda x, y: x['text']))\n\n\ntext_features = text_vectorize_layer(text_input) # apply this \"function TextVectorization layer\" to lyrics_input\ntext_features = layers.Embedding(size_vocabulary, output_dim = 3, name=\"text_embedding\")(text_features) # embed words\ntext_features = layers.Dropout(0.2)(text_features) # prevent overfitting\ntext_features = layers.GlobalAveragePooling1D()(text_features)\ntext_features = layers.Dropout(0.2)(text_features) # prevent overfitting\ntext_features = layers.Dense(32, activation='relu')(text_features)\n\n\noutput = layers.Dense(1, name=\"fake\")(text_features) # binary classification\n\n\n# text model\ntext_model = keras.Model(\n    inputs = [text_input],\n    outputs = output\n)\n\n\ntext_model.summary()\n\nModel: \"model_1\"\n\n\n_________________________________________________________________\n\n\n Layer (type)                Output Shape              Param #   \n\n\n=================================================================\n\n\n text (InputLayer)           [(None, 1)]               0         \n\n\n                                                                 \n\n\n text_vectorization_1 (TextV  (None, 500)              0         \n\n\n ectorization)                                                   \n\n\n                                                                 \n\n\n text_embedding (Embedding)  (None, 500, 3)            6000      \n\n\n                                                                 \n\n\n dropout_2 (Dropout)         (None, 500, 3)            0         \n\n\n                                                                 \n\n\n global_average_pooling1d_1   (None, 3)                0         \n\n\n (GlobalAveragePooling1D)                                        \n\n\n                                                                 \n\n\n dropout_3 (Dropout)         (None, 3)                 0         \n\n\n                                                                 \n\n\n dense_1 (Dense)             (None, 32)                128       \n\n\n                                                                 \n\n\n fake (Dense)                (None, 1)                 33        \n\n\n                                                                 \n\n\n=================================================================\n\n\nTotal params: 6,161\n\n\nTrainable params: 6,161\n\n\nNon-trainable params: 0\n\n\n_________________________________________________________________\n\n\n\nkeras.utils.plot_model(text_model) # model architecture\n\nYou must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n\n\n\n\n\nText Model Architecture\n\n\n\ntext_model.compile(optimizer=\"adam\",\n              loss = keras.losses.BinaryCrossentropy(from_logits=True), # binary classification\n              metrics=[\"accuracy\"])\n\n#| id: RAAe1xlykWPa\n#| colab: {base_uri: 'https://localhost:8080/'}\n#| outputId: 2ccc3840-14d4-4392-ff32-44b9ff3d767f\ntext_history = text_model.fit(train, \n                    validation_data=val,\n                    epochs = 50, \n                    verbose = False)\n#| id: E9jzEJS1kZM3\n#| colab: {base_uri: 'https://localhost:8080/', height: 609}\n#| outputId: 7ed8b00f-7623-4c1f-86ec-1aa930be2ddd\nplot_val(text_history, 0.97, title = 'Model Performance with Text Input')\n\n\n\nPlot2\n\n\nWe see a very similar trend with using article text only to classify an article as fake news or real news. The validation and training accuracy both reach over 97% accuracy over a number of epochs, and the validation accuracy is consistently better than the training accuracy, which showcases that there is no presence of overfitting.\n\n\n\nNow, we can use both title and text and evaluate its performance. Since we wrote variables for both models individually, we can reuse this code and just change the end to combine both models into one. To do this, we just need to use a layers.concatenate layer to add these two inputs together.\n\n# title\ntitle_input = keras.Input(\n    shape=(1,),\n    name = \"title\", # same name as the dictionary key in the dataset\n    dtype = \"string\"\n)\n\n# text\ntext_input = keras.Input(\n    shape=(1,),\n    name = \"text\", # same name as the dictionary key in the dataset\n    dtype = \"string\"\n)\n\n# share an embedding layer with 16 elements\nshared_embedding = layers.Embedding(size_vocabulary, 16)\n\n\ntitle_features = title_vectorize_layer(title_input) # apply this \"function TextVectorization layer\" to lyrics_input\ntitle_features = shared_embedding(title_features)\ntitle_features = layers.Dropout(0.2)(title_features)\ntitle_features = layers.GlobalAveragePooling1D()(title_features)\ntitle_features = layers.Dropout(0.2)(title_features)\ntitle_features = layers.Dense(32, activation='relu')(title_features)\n\ntext_features = text_vectorize_layer(text_input) # apply this \"function TextVectorization layer\" to lyrics_input\ntext_features = shared_embedding(text_features)\ntext_features = layers.Dropout(0.2)(text_features)\ntext_features = layers.GlobalAveragePooling1D()(text_features)\ntext_features = layers.Dropout(0.2)(text_features)\ntext_features = layers.Dense(32, activation='relu')(text_features)\n\n\nmain = layers.concatenate([title_features, text_features], axis = 1)\n\n\nmain = layers.Dense(32, activation='relu')(main)\noutput = layers.Dense(1, name=\"fake\")(main)\n\n\n# title + text model\ntt_model = keras.Model(\n    inputs = [title_input, text_input],\n    outputs = output\n)\n\n\ntt_model.summary()\n\nModel: \"model_2\"\n\n\n__________________________________________________________________________________________________\n\n\n Layer (type)                   Output Shape         Param #     Connected to                     \n\n\n==================================================================================================\n\n\n title (InputLayer)             [(None, 1)]          0           []                               \n\n\n                                                                                                  \n\n\n text (InputLayer)              [(None, 1)]          0           []                               \n\n\n                                                                                                  \n\n\n text_vectorization (TextVector  (None, 500)         0           ['title[0][0]']                  \n\n\n ization)                                                                                         \n\n\n                                                                                                  \n\n\n text_vectorization_1 (TextVect  (None, 500)         0           ['text[0][0]']                   \n\n\n orization)                                                                                       \n\n\n                                                                                                  \n\n\n embedding (Embedding)          (None, 500, 16)      32000       ['text_vectorization[1][0]',     \n\n\n                                                                  'text_vectorization_1[1][0]']   \n\n\n                                                                                                  \n\n\n dropout_4 (Dropout)            (None, 500, 16)      0           ['embedding[0][0]']              \n\n\n                                                                                                  \n\n\n dropout_6 (Dropout)            (None, 500, 16)      0           ['embedding[1][0]']              \n\n\n                                                                                                  \n\n\n global_average_pooling1d_2 (Gl  (None, 16)          0           ['dropout_4[0][0]']              \n\n\n obalAveragePooling1D)                                                                            \n\n\n                                                                                                  \n\n\n global_average_pooling1d_3 (Gl  (None, 16)          0           ['dropout_6[0][0]']              \n\n\n obalAveragePooling1D)                                                                            \n\n\n                                                                                                  \n\n\n dropout_5 (Dropout)            (None, 16)           0           ['global_average_pooling1d_2[0][0\n\n\n                                                                 ]']                              \n\n\n                                                                                                  \n\n\n dropout_7 (Dropout)            (None, 16)           0           ['global_average_pooling1d_3[0][0\n\n\n                                                                 ]']                              \n\n\n                                                                                                  \n\n\n dense_2 (Dense)                (None, 32)           544         ['dropout_5[0][0]']              \n\n\n                                                                                                  \n\n\n dense_3 (Dense)                (None, 32)           544         ['dropout_7[0][0]']              \n\n\n                                                                                                  \n\n\n concatenate (Concatenate)      (None, 64)           0           ['dense_2[0][0]',                \n\n\n                                                                  'dense_3[0][0]']                \n\n\n                                                                                                  \n\n\n dense_4 (Dense)                (None, 32)           2080        ['concatenate[0][0]']            \n\n\n                                                                                                  \n\n\n fake (Dense)                   (None, 1)            33          ['dense_4[0][0]']                \n\n\n                                                                                                  \n\n\n==================================================================================================\n\n\nTotal params: 35,201\n\n\nTrainable params: 35,201\n\n\nNon-trainable params: 0\n\n\n__________________________________________________________________________________________________\n\n\n\nkeras.utils.plot_model(tt_model)\n\nYou must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n\n\n\n\n\nTitle-Text Model Architecture\n\n\n\ntt_model.compile(optimizer=\"adam\",\n              loss = keras.losses.BinaryCrossentropy(from_logits=True), # binary classification\n              metrics=[\"accuracy\"])\n\n#| id: 2Vy4ZiqMqEiW\ntt_history = tt_model.fit(train, \n                    validation_data=val,\n                    epochs = 50, \n                    verbose = False)\n#| id: S35tT4KxqLaP\n#| colab: {base_uri: 'https://localhost:8080/', height: 609}\n#| outputId: 19dd5a3e-50fc-4914-89a5-48ab12b55382\nplot_val(tt_history, 0.97, title = 'Model Performance with Text Input')\n\n\n\nPlot3\n\n\nThis model also performs insanely well, it seems that the training and validation accuracy both reach extremely closely to 100% accuracy. There is no overfitting in this case, but it does seem to be nearing a perfect classification.\n\n\n\nAs you can see, all three of these models performed exceptionally well. Which one do we choose? In this case, I am going to be looking at the summary statistics for the validation accuracies for the last 30 epochs to see which one I would like to use.\n#| id: IcsbBz86srfj\n#| colab: {base_uri: 'https://localhost:8080/'}\n#| outputId: 217480cb-8920-496b-e2c0-b67b73ecc4a1\nprint(\"Title:\")\nprint(pd.Series(title_history.history['val_accuracy'][-30:]).describe())\nprint(\"\\nText:\")\nprint(pd.Series(text_history.history['val_accuracy'][-30:]).describe())\nprint(\"\\nTitleText:\")\nprint(pd.Series(tt_history.history['val_accuracy'][-30:]).describe())\n We can see that the mean and the median are both at their peak with the model containing both the title and text information. The miniumum accuracy of the last model does not even go below 99.9% accuracy, which the other two models cannot even hit. Therefore, we will use the last model on the unseen test data in hopes of good performance.\n\n\n\n\nLet us look at the performance on the unseen test data. The test data is downloaded as follows:\n\n# load test data\ntest_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true\"\ntest_news = pd.read_csv(test_url)\ntest_news.columns = ['id', 'title', 'text', 'fake']\n\n\ntest_data = make_dataset(test_news, ['title', 'text'], 'fake')\n\n#| id: 2b0JzXvTv5dV\n#| colab: {base_uri: 'https://localhost:8080/'}\n#| outputId: c3b7fdd0-aeeb-496b-ad56-1af31d465029\ntest_loss, test_acc = tt_model.evaluate(test_data)\n\nprint('Test Loss: ', test_loss)\nprint('Test Accuracy: ', test_acc)\n\n\n\nTest Results\n\n\nAccording to the test data, we are able to gain above 97% accuracy, which is amazingly good. It would be right about 97% of the time, which is helpful especially when it comes to misinformation.\n\n\n\nNow, we will look a bit deeper into the embedding layer for this model.\n#| id: HUShOoyf440V\nvocab = text_vectorize_layer.get_vocabulary() # keeps track of mapping from word to integer\n#| id: V_1Y1y-Z5Qxz\ntext_weights = tt_model.get_layer(\"embedding\").get_weights()[0] # weights for embedding each of the 2000 words\n#| id: SHrs-Yxo5Z-5\n#| colab: {base_uri: 'https://localhost:8080/'}\n#| outputId: d749721a-93b6-4901-b553-feca3508c4ca\ntext_weights.shape\n#| id: i_2UBndX5XZN\nfrom sklearn.decomposition import PCA \n# https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n# principal components analysis - \n# project things to lower dimension such that the variance of the dataset is most preserved\n\npca = PCA(n_components=2)\nweights = pca.fit_transform(text_weights)\n#| id: SoQgrX1D5fhX\nembedding_df = pd.DataFrame({\n    'word': vocab,\n    'x0': text_weights[:, 0],\n    'x1': text_weights[:, 1]\n}) # on PCA axes\n#| id: pfmVqcYR5kxq\n#| colab: {base_uri: 'https://localhost:8080/', height: 542}\n#| outputId: f1271819-bae4-48db-d6e8-06364184bc90\nimport plotly.express as px\nfig = px.scatter(embedding_df,\n                 x='x0',\n                 y='x1',\n                 size=[2]*len(embedding_df),\n                 #size_max = 2,\n                 hover_name = 'word' \n                 )\nfig.show()\n\nFrom this visualization, we can see that the embedding goes to two different directions on the two major principal component axes. On the bottom left, we have words such as \"gop\", \"rep\", \"reportedly\". These touch upon politics (i.e. elections), but what ties these words together is this word \"21wiretv\". After doing some research, it is found that \"21wiretv\" is a journalism website that has a reputation of conspiracy theories and/or hoaxes. This leads me to believe that this side errs on fake news. On the other hand, we have terms such as \"trumps\", \"obamas\", \"partys\". It talks about politics, but particularly about presidents. We also see words like \"opinions\", which leads me to believe that this end is more about real news because they are willing to state opinion as opposed to deceive people by saying it is fact.\n\n\n\nFrom this article, we have learned to process a dataset with text inputs so that it is fit for training through neural networks. We then learned how to use the functional API to create three different models. Then, we sought to interpret the embedding to see how the model learned how to classify fake news. This most definitely can be used for many other applications that use text, so go out and explore this world of working with text data!"
  },
  {
    "objectID": "posts/HW4/index.html",
    "href": "posts/HW4/index.html",
    "title": "Classifying Pets through Convolutional Neural Networks with Tensorflow",
    "section": "",
    "text": "The CNN classifies this as a good doggo\nFor this blog post, we will be looking into image classification of pets using Python’s Tensorflow package to create a convolutional neural network (CNN). In this post, you will learn:\nWe will be working with an image dataset that works with images of cats and dogs. The CNN will take these input images and train on these images to get a model that would predict and classify any new images of cats or dogs."
  },
  {
    "objectID": "posts/HW4/index.html#model-1",
    "href": "posts/HW4/index.html#model-1",
    "title": "Classifying Pets through Convolutional Neural Networks with Tensorflow",
    "section": "Model 1",
    "text": "Model 1\nWith the benchmark in mind, let us start by making our first model. We will first start off with 3 Convolution layers and MaxPooling layers, adding a dropout layer in hopes to prevent overfitting, and flatten out the nodes and to classify the input as either a cat or a dog. Read more about the different types of layers for a convolutional neural network (CNN) here. We also need to compile() the model, which will determine what metrics and optimizer we use to evaluate the performance of the model on the training and validation data. Based on literature and recommendations, this model will use the Adam optimizer for stochastic gradient descent, and evaluate the performance through binary cross-entropy and accuracy.\n\nmodel1 = tf.keras.Sequential([\n    layers.Conv2D(16, (3, 3), activation='relu', input_shape = (160, 160, 3)),\n    layers.MaxPooling2D(),\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D(),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D(),\n    layers.Dropout(0.05),\n    layers.Flatten(),\n    layers.Dense(32, activation = 'relu'),\n    layers.Dense(1, activation = 'sigmoid')\n])\n\nmodel1.compile(optimizer = 'adam', loss = tf.keras.losses.BinaryCrossentropy(),\n              metrics = ['accuracy'])\n\nmodel1.summary()\n\n\n\nModel 1 Summary\n\n\nhistory = model1.fit(train_dataset, \n                     epochs=20, \n                     validation_data=validation_dataset)\n\ndef acc_eval(history, title, threshold):\n    '''\n    creates a plot of accuracy over epoches.\n    @ input:\n    - history: the neural network\n    - title (str): Title of Visualization\n    @ output:\n    - plot: plot of accuracy\n    '''\n    plt.plot(history.history['accuracy'], label = 'training')\n    plt.plot(history.history['val_accuracy'], label = 'validation')\n    plt.legend()\n    plt.title(title)\n    plt.xticks(np.arange(0, 21, 2))\n    plt.xlabel(\"epoch\")\n    plt.ylabel(\"accuracy\")\n    plt.axhline(y = threshold, color = 'r', linestyle = 'dashed')\n    plt.show()\n\nacc_eval(history, \"Model 1\", 0.52)\n\n\n\nModel 1 Performance\n\n\nAs seen from the output of the model fit and the visualization, we can see that the output stablized between 55% and 60% validation accuracy during training. This is only slightly better than the baseline accuracy of 50%. When evaluating for overfitting, we need to look at the difference between the training accuracy and the validation accuracy. Since the training accuracy has trained to be more than 90% accurate while the validation accuracy stayed below 60%, it is reasonable to say that the model is overfitting to the training data."
  },
  {
    "objectID": "posts/HW4/index.html#model-2",
    "href": "posts/HW4/index.html#model-2",
    "title": "Classifying Pets through Convolutional Neural Networks with Tensorflow",
    "section": "Model 2",
    "text": "Model 2\nThis is the second model. It will contain the same architecture as that of Model 1 and add on the functions that we have just created.\n\nmodel2 = tf.keras.Sequential([\n    flip,\n    rotate,\n    layers.Conv2D(16, (3, 3), activation='relu', input_shape = (160, 160, 3)),\n    layers.MaxPooling2D(),\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D(),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D(),\n    layers.Dropout(0.01),\n    layers.Flatten(),\n    layers.Dense(32, activation = 'relu'),\n    layers.Dense(1, activation = 'sigmoid')\n])\n\nmodel2.compile(optimizer = 'adam', loss = tf.keras.losses.BinaryCrossentropy(),\n              metrics = ['accuracy'])\n\nhistory = model2.fit(train_dataset, \n                     epochs=20, \n                     validation_data=validation_dataset)\nmodel2.summary()\n\n\n\nModel 2 Summary\n\n\nacc_eval(history, \"Model 2\", 0.55)\n\n\n\nModel 2 Performance\n\n\nWe can see that the validation accuracy consistently falls between 56% and 62%. This is a slight improvement compared to the first model. In this case, we still see slight overfitting, in that the training accuracy is still somewhat higher than the validation accuracy, although the difference is not as drastic compared to the first model. The training accuracy improved much slower in this model compared to the first model. Let’s see how we can further improve this model"
  },
  {
    "objectID": "posts/HW4/index.html#model-3",
    "href": "posts/HW4/index.html#model-3",
    "title": "Classifying Pets through Convolutional Neural Networks with Tensorflow",
    "section": "Model 3",
    "text": "Model 3\n\nmodel3 = tf.keras.Sequential([\n    preprocessor,\n    flip,\n    rotate,\n    layers.Conv2D(16, (3, 3), activation='relu', input_shape = (160, 160, 3)),\n    layers.MaxPooling2D(),\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D(),\n    layers.Dropout(0.2),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D(),\n    layers.Dropout(0.3),\n    layers.Flatten(),\n    layers.Dense(32, activation = 'relu'),\n    layers.Dense(1, activation = 'sigmoid')\n])\n\nmodel3.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.0006), loss = tf.keras.losses.BinaryCrossentropy(),\n              metrics = ['accuracy'])\n\nhistory = model3.fit(train_dataset, \n                     epochs=20, \n                     validation_data=validation_dataset)\nmodel3.summary()\n\n\n\nModel 3 Summary\n\n\nacc_eval(history, \"Model 3\", 0.70)\n\n\n\nModel 3 Performance\n\n\nFrom this model, we can see that the preprocessing has helped a lot. The validation accuracy fluctuates from 70% to 73% as the model trains through more epochs. This performs well compared to the baseline accuracy of 50%, as well as the accuracy of 52% for the first model. In order to prevent overfitting, we increased the percentage of observations to drop out in the dropout() layer. In this case, overfitting is not an issue in comparison to the first two models, as the difference between the training and validation accuracy is not as drastic."
  },
  {
    "objectID": "posts/HW4/index.html#model-4",
    "href": "posts/HW4/index.html#model-4",
    "title": "Classifying Pets through Convolutional Neural Networks with Tensorflow",
    "section": "Model 4",
    "text": "Model 4\n\nmodel4 = tf.keras.Sequential([\n    preprocessor,\n    flip,\n    rotate,\n    base_model_layer,\n    layers.GlobalMaxPooling2D(),\n    layers.Dropout(0.2),\n    layers.Dense(1, activation = 'sigmoid')\n])\n\nmodel4.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.0006), loss = tf.keras.losses.BinaryCrossentropy(),\n              metrics = ['accuracy'])\n\nmodel4.summary()\n\n\n\nModel 4 Summary\n\n\nhistory = model4.fit(train_dataset, \n                     epochs=20, \n                     validation_data=validation_dataset)\nacc_eval(history, \"Model 4\", 0.95)\n\n\n\nModel 4 Performance\n\n\nWith this model, we can see that the validation accuracy reaches above the desired threshold of 95% after the second epoch. This validation accuracy is substantially higher than the first three models that we made. This is also the first where the validation accuracy exceeds that of the corresponding training accuracy. In this case, there is no overfitting, as the training accuracy does not differ from the validation accuracy greatly."
  },
  {
    "objectID": "posts/HW3/index.html",
    "href": "posts/HW3/index.html",
    "title": "Creating a Message Bank with Flask",
    "section": "",
    "text": "This post will look into creating a message bank using Flask. This message bank will allow for users to write in their own input, and to submit it to a database. They will also have the option to view past messages that have already been stored by the database. This post will direct you through the creation of this website. From this post, you will learn:\nLet’s get started with building this message bank!\nNOTE: All of these files take its inspiration from a lecture with examples in PIC 16B W23."
  },
  {
    "objectID": "posts/HW3/index.html#templates",
    "href": "posts/HW3/index.html#templates",
    "title": "Creating a Message Bank with Flask",
    "section": "Templates",
    "text": "Templates\nNow that you have your app.py file, you now need to create templates that would act as what you would want to display on your website for given pages. In your directory, create a folder called templates, and begin to add template files in the form of .html.\nFor starters, here are some tags that will be helpful for you to better understand what is going on in HTML notation:\n\n<title>: sets the title of the webpage, displayed on the browser tab.\n<nav>: section for navigation links\n<h1>: heading\n<ul>: unbulleted list\n<li>: list element\n<a>: link\n<section>: defines section\n<header>: heading of section\n\nIf you would like to learn more, please check out this link.\n\nbase.html\nIn this .html file, this will provide the basic template that the pages will follow, so that you don’t have to repeat the same code over and over again for different web pages of this website. Here is the code for my base.html. I will walk through some of the logic of my code.\n<!doctype html>\n<!-- <link rel=\"stylesheet\" href=\"{{ url_for('static', filename='style.css') }}\"> -->\n<title>{% block title %}{% endblock %} - PIC16B Message Bank</title>\n<nav>\n  <h1 class=\"title\">Message Bank</h1>\n  <!-- <b>Navigation:</b> -->\n  <ul>\n      <li><a href=\"{{ url_for('submit') }}\">Submit A Message</a></li>\n      <li><a href=\"{{ url_for('view')}}\">View Random Messages</a></li>\n  </ul>\n</nav>\n<section class=\"content\">\n  <header>\n    {% block header %}{% endblock %}\n  </header>\n  {% block content %}{% endblock %}\n</section>\nFirst, I set my title for the webpage as {% block title %}{% endblock %} - PIC16B Message Bank. This brings us to some interesting notation, which is {% and %}. The purpose of those characters is to make anything within them a part of a template. So if another HTML file wants to overwrite the title in base.html, it can write anything in between {% block title %} and {% endblock %} to replace it. We will see this later.\nThen, we get to the navigation, which contains two links in a list to either Submit A Message or View Random Messages, each linked with the respective page that they should redirect to.\nThen, within the content section, we see again two more templates, one for the header and one for the content. We will see how we will alter this with different pages.\n\n\nsubmit.html\nThe first page that we will need to make is a submit.html, so that users can write a message and submit it to the database. However, we do not need to rewrite the code written in base.html, but expand upon that by writing with the template blocks. Here is the code:\n{% extends 'base.html' %}\n\n{% block title %} Submit a Message {% endblock %}\n\n{% block header %}\n  <h1> Submit a Message </h1>\n{% endblock %}\n\n{% block content %}\n  <form method=\"post\">\n    <label for=\"message\">Your Message:</label>\n      <br>\n    <textarea cols=\"80\" rows=\"10\" id=\"message\" type=\"text\" name=\"message\" required>\n</textarea>\n    <!--<input class = \"msg\" name=\"message\" id=\"message\">-->\n      <br>\n      <br>\n    <label for=\"user\">Your Name or Handle:</label>\n    <input name=\"user\" id=\"user\" required>\n      <br>\n      <br>\n      <div class=\"text-center\">\n    <input type=\"submit\" value=\"Submit message\">\n      </div>\n  </form>\n\n  {% if name %}\n<p><b>Thanks for submitting a message! Here is your message:</b></p>\n<p class=\"message\">&emsp; \"{{message}}\" </p>\n  {% endif %}\n\n  {% if error %}\n    <br>\n    Please fill out both your message and your handle.\n  {% endif %}\n{% endblock %}\nAt the top, we see this code: {% extends 'base.html'%}. This lets our app know that this will build off of the template made from base.html. As I said earlier, we will only need to change the contents within the blocks, which is what we did. We change the title and header to display Submit a Message. We also change the content block with a <form> that will take in user input and allow them to submit that message.\nWe see that there are two other blocks containing if statements. This will be run only if the condition is held, which we will cover how to control when this happens.\n\n\nview.html\nThe other page we need to create is the page to view other people’s submissions. It will follow the same base.html template, but will display different information than submit.html. Here is the code:\n{% extends 'base.html' %}\n\n{% block header %}\n  <h1>{% block title %}Some Cool Messages{% endblock %}</h1>\n{% endblock %}\n\n{% block content %}\n  {% if msgs %}\n<p>Here are some of the messages people have sent!</p>\n    <br>\n    <br>\n    {% for msg in msgs %}\n        <div> \"{{ msg[1] }}\"\n            <br> &emsp;- {{ msg[0] }}\n    </div>\n    <br>\n    {% endfor %}\n  {% endif %}\n\n  {% if error %}\n    <br>\n    We could not display any messages. This is because there are no messages stored.\n  {% endif %}\n{% endblock %}\nThe logic is similar to that of submit.html, but now in the content block, we see that there is a for loop that will display messages if msgs is available.\nThese are the basic files that are necessary to run the file. Now, we need to work on piecing them all together so that app.py can connect to these pages and display them correctly."
  },
  {
    "objectID": "posts/HW3/index.html#creation-and-getting-of-database",
    "href": "posts/HW3/index.html#creation-and-getting-of-database",
    "title": "Creating a Message Bank with Flask",
    "section": "Creation and Getting of Database",
    "text": "Creation and Getting of Database\nimport sqlite3\n\nDB_NAME = './messages_db.sqlite'\ndef get_message_db():\n    '''\n    Retrieves the message database\n    @ output:\n    - g.message_db: a database storing messages\n    '''\n    try:\n        # returns a database\n        return g.message_db\n    except:\n        # connect to a database\n        with sqlite3.connect(DB_NAME) as conn:\n            g.message_db = conn\n            \n            # create a table if it doesn't exist\n            cursor = conn.cursor()\n            query = '''\n                    CREATE TABLE IF NOT EXISTS messages (\n                    id INTEGER PRIMARY KEY,\n                    username TEXT,\n                    message TEXT);\n                    '''\n            cursor.execute(query)\n            # return the database\n            return g.message_db\nWhat this function does is retrieve the database. If it does exist, then it will return the database. If it doesn’t exist, it will create a connection and create a table within the database and return the database. This is a function that will be used in the other functions to ensure that the same table will be used when inserting messages and viewing random messages."
  },
  {
    "objectID": "posts/HW3/index.html#add-messages-to-table",
    "href": "posts/HW3/index.html#add-messages-to-table",
    "title": "Creating a Message Bank with Flask",
    "section": "Add Messages to Table",
    "text": "Add Messages to Table\ndef insert_message(request):\n    '''\n    inserts a message into the database\n    @ input:\n    - request: URL requesting data from\n    @ output:\n    - message (str): the message the user input\n    - handle (str): the handle of the user\n    '''\n    # obtain the request and the information\n    message = request.form['message']\n    handle = request.form['user']\n    \n    # get the table, and insert it into the table\n    with get_message_db() as conn:\n        cursor = conn.cursor()\n        cursor.execute(\"INSERT INTO messages (id, username, message) VALUES ((SELECT COUNT(*) FROM messages) + 1, ?, ?)\", (handle, message))\n        conn.commit() # save changes\n    # return message and handle\n    return message, handle\nThis function will take a request, assuming that a user has submitted a message, and output the message and handle. This function will connect with the submission page of the website (see the submit() function). When a user provides input for both the message and the handle, the request will contain both of these inputs, and we will run insert_message() to store this into the database. The function itself will also take this message and handle and add it to the table through a SQL query, using get_message_db() in order to establish our connection.\nThis function also provides the output necessary to write a confirmation message to the user that their message was put into the database. We write the insert_message() to a local variable that contains the message and the handle. These become keyword arguments to render_template() to activate the if name statement within the submit.html so that the page would thank them for a message and spit out their message back to them to show what they have submitted (see submit.html)."
  },
  {
    "objectID": "posts/HW3/index.html#get-random-messages",
    "href": "posts/HW3/index.html#get-random-messages",
    "title": "Creating a Message Bank with Flask",
    "section": "Get Random Messages",
    "text": "Get Random Messages\ndef random_messages(n):\n    '''\n    picks at most n random messages from the message database.\n    @ input:\n    - n (int): how many messages to display\n    @ output:\n    - msgs (list): list of random messages\n    '''\n    # get the database\n    with get_message_db() as conn:\n        cursor = conn.cursor()\n        # select the random username and message\n        cursor.execute(\"SELECT username, message FROM messages ORDER BY RANDOM() LIMIT {0};\".format(n))\n        msgs = cursor.fetchall()\n        # return list\n        return msgs\nThis function takes in a number n, and obtains n random messages, which is stored on a list. This connects with the view() function, where the number of messages to view is hard coded as 5. Using get_messages_db() to obtain the connection, we use a SQL query to randomly order the rows in the table and getting the top n messages. In the view() function, it is stored within a local variable called msgs, which is a list containing tuples with the message and handle. When render_template() is called with the argument msgs, we use a for loop cycle through the n messages and to display them on different <div> tags (see view.html).\nThese functions are all helpful in the web app, allowing for a database to be utilized to store messages created by users and to retrieve said messages for viewers’ entertainment."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post for my blog.\n\nThis was originally built for a class, but now it expands to cover my own personal projects as well. Feel free to browse around, and view my contact information on the About page."
  },
  {
    "objectID": "posts/HW1/index.html",
    "href": "posts/HW1/index.html",
    "title": "SQL Queries and Plotly Visualizations",
    "section": "",
    "text": "In this blog post, we will be diving deeper into data visualization, specifically with plotly. By the end of this post, you will be able to:\nThis article is a good exercise for the data visualization process. I will first go through the steps, and then provide additional examples."
  },
  {
    "objectID": "posts/HW1/index.html#step1",
    "href": "posts/HW1/index.html#step1",
    "title": "SQL Queries and Plotly Visualizations",
    "section": "Create a Database",
    "text": "Create a Database\nFirst, we need to create a database that will store all the necessary data we need in order to create our visualizations. This is done through the sqlite3 library. The .csv files were downloaded as toy datasets from a class curriculum.\n\nimport sqlite3\nimport pandas as pd\n\n# initialize a connection to new database\nconn = sqlite3.connect(\"weather.db\")\n\ndef create_table(df, out_name, **kwargs):\n    '''\n    Input:\n    - df (str): path of the data frame\n    - out_name (str): name of output table\n    - **kwargs: keyword arguments for to_sql\n    '''\n    tbl = pd.read_csv(df) # reads csv\n    tbl.to_sql(out_name, **kwargs) # transfers csv to sql database\n\n# repetitive keywords\nkeywords = {'con': conn, 'index': False, 'if_exists': 'replace'}\n\n# create 3 tables\ncreate_table(\"temps_stacked.csv\", \"temperatures\", **keywords)\ncreate_table(\"countries.csv\", \"countries\", **keywords)\ncreate_table(\"station-metadata.csv\", \"stations\", **keywords)\n\n# verify that 3 tables are in the database\ncursor = conn.cursor()\ncursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\nprint(cursor.fetchall())\nconn.close() # close connection\n\n[('temperatures',), ('countries',), ('stations',)]\n\n\nAfter this code is run, we have a database labelled weather.db, containing 3 tables temperatures, countries, and stations. Here is a brief description of each table:\n\ntemperatures: a list of temperature recordings of stations every month and year.\ncountries: country names, their respective IDs, and abbreviations\nstations: station names, what country they are in, as well as latitude and longitude."
  },
  {
    "objectID": "posts/HW1/index.html#step2",
    "href": "posts/HW1/index.html#step2",
    "title": "SQL Queries and Plotly Visualizations",
    "section": "Extract the Necessary Data",
    "text": "Extract the Necessary Data\nNow, we need to achieve the necessary data and compile it into a file that we will use for the visualization. Each of the three tables contain important information and relationships that would be helpful if they were all merged together. We will write a function that takes a SQL query to take these tables, extract the necessary columns from each, and output a dataframe with all the information to create a visualization. I would encourage you to look at each of the tables and their columns to get a sense of what information there is. For the sake of this demonstration, this will be given to you. We want\n\nThe station name\nThe latitude of the station\nThe longitude of the station\nThe country where the station is located in\nThe year the reading was taken\nThe month the reading was taken\nThe average temperature at this station at the given year and month\n\n\nConstruct a SQL Query\nThis function we are able to make will only work if we extract the right information from the SQL query. This subsection will inform you of the SQL syntax, and some common tips.\n\n1. SQL Syntax\nThere are various keywords used in SQL that form a query. Here is a brief overview of the commands necessary for reading a table.\n\nSELECT - extracts columns from table\nFROM - used in conjunction with SELECT to specifies the table you are extracting from\n(FULL|INNER|RIGHT|LEFT) JOIN - joins two tables together (the four keywords denote if a table keeps all their data)\nON - used in conjunction with JOIN keyword to specify the columns that link the two tables together\nWHERE - subsets the table to a given specification\nGROUP BY - similar to pandas.groupby(), groups table by specific columns\nHAVING - used in conjunction with GROUP BY to specify more subsetting within groups.\nORDER BY - rearranges table to order by specific columns\nDESC - used in conjunction with ORDER BY to specify reverse ordering\nLIMIT - limits the number of rows outputted\n\n\n\n2. Common Tips\nThere are a couple queries that are commonly used in SQL. Here are some of them, and I will explain it all in detail\n\nExample 1:\nSELECT * FROM temperatures;\nThe * character denotes a wildcard, meaning all columns. This statement will extract all columns from the temperatures table.\n\n\nExample 2:\nSELECT year, month, temperature FROM temperatures\nWHERE month = 12;\nGet the year, month, and temperature from the temperatures table, subsetted only to the month of December.\n\n\nExample 3:\nSELECT t.id, t.temperature, s.name FROM temperatures t\nINNER JOIN stations s\nON s.id = t.id;\nGet the id and temperature from temperatures and name from stations, aliased as t and s, respectively.\n\n\nExample 4:\nSELECT t.id, t.temperature, s.name FROM temperatures t\nINNER JOIN stations s\nON s.id = t.id\nWHERE t.temperature > 20;\nThe same query as Example 3, but subsetted by temperatures greater than 20.\n\n\nExample 5:\nSELECT t.id, AVG(t.temperature), t.year, s.name FROM temperatures t\nINNER JOIN stations s\nON s.id = t.id\nGROUP BY t.year;\nThe same query as Example 3, but now calculating the average temperature grouped by year.\n\n\n\n3. Constructing the Query\nNow that we have a better understanding of SQL, we will have to construct the query that will serve as the basis for this function. Here are the specs for the query, for your convenience:\n\nThe station name\nThe latitude of the station\nThe longitude of the station\nThe country where the station is located in\nThe year the reading was taken\nThe month the reading was taken\nThe average temperature at this station at the given year and month We can start by writing down the columns in a SQL format. However, these variables are not all found in the same table, so we need to join tables together in order for the initial query to work. Look into the tables in your own time to see that the relation key between the temperatures and stations tables is by their ID, and the relationship between temperatures and countries is by a subset of the ID corresponding with the country’s FIPS 10-4 code’.\n\nSELECT s.Name, s.latitude, s.longitude, c.Name, t.year, t.month, t.temperature\nFROM temperatures t\nLEFT JOIN stations s ON t.id = s.id\nLEFT JOIN countries c ON SUBSTRING(t.id, 1, 2) = c.[FIPS 10-4]\nHowever, one of the things that we want to do is to specify subsetting by country, year and month. We can do this by adding the WHERE clause.\nSELECT s.Name, s.latitude, s.longitude, c.Name, t.year, t.month, t.temperature\nFROM temperatures t\nLEFT JOIN stations s ON t.id = s.id\nLEFT JOIN countries c ON SUBSTRING(t.id, 1, 2) = c.[FIPS 10-4]\nWHERE c.NAME == \"<country name>\"\nAND t.year < <year_end>\nAND t.year > <year_begin>\nAND t.month == <month>\nThis is the framework of the query we will use for the function.\n\n\n\nConstructing the Function\n\nimport sqlite3\nimport pandas as pd\nimport numpy as np\n\nconn = sqlite3.connect('weather.db')\ndef query_climate_database(country, year_begin, year_end, month):\n    '''\n    runs a SQL query that obtains temperature data for stations in a certain country for given years and months.\n    @input:\n    - country (str): (Official) Country Name \n    - year_begin (int): starting year to look for\n    - year_end (int): end year to look for\n    - month (int): which month to look at\n    @output:\n    - df: takes SQL results as a dataframe\n    '''\n\n    # SQL query\n    cmd = '''\n    SELECT s.Name, s.latitude, s.longitude, c.Name, t.year, t.month, t.temp\n    FROM temperatures t\n    LEFT JOIN stations s ON t.id = s.id\n    LEFT JOIN countries c ON SUBSTRING(t.id, 1, 2) = c.[FIPS 10-4]\n    WHERE c.NAME == \\\"{0}\\\"\n    AND t.year <= {2} AND t.year >= {1}\n    AND t.month == {3}'''.format(country, year_begin, year_end, month)\n\n    # establish connection\n    \n    return pd.read_sql_query(cmd, conn) # return data frame\n\nquery_climate_database(\"India\", 1980, 2020, 1)\n\n\n\n\n\n  \n    \n      \n      NAME\n      LATITUDE\n      LONGITUDE\n      Name\n      Year\n      Month\n      Temp\n    \n  \n  \n    \n      0\n      PBO_ANANTAPUR\n      14.583\n      77.633\n      India\n      1980\n      1\n      23.48\n    \n    \n      1\n      PBO_ANANTAPUR\n      14.583\n      77.633\n      India\n      1981\n      1\n      24.57\n    \n    \n      2\n      PBO_ANANTAPUR\n      14.583\n      77.633\n      India\n      1982\n      1\n      24.19\n    \n    \n      3\n      PBO_ANANTAPUR\n      14.583\n      77.633\n      India\n      1983\n      1\n      23.51\n    \n    \n      4\n      PBO_ANANTAPUR\n      14.583\n      77.633\n      India\n      1984\n      1\n      24.81\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      3147\n      DARJEELING\n      27.050\n      88.270\n      India\n      1983\n      1\n      5.10\n    \n    \n      3148\n      DARJEELING\n      27.050\n      88.270\n      India\n      1986\n      1\n      6.90\n    \n    \n      3149\n      DARJEELING\n      27.050\n      88.270\n      India\n      1994\n      1\n      8.10\n    \n    \n      3150\n      DARJEELING\n      27.050\n      88.270\n      India\n      1995\n      1\n      5.60\n    \n    \n      3151\n      DARJEELING\n      27.050\n      88.270\n      India\n      1997\n      1\n      5.70\n    \n  \n\n3152 rows × 7 columns"
  },
  {
    "objectID": "posts/HW1/index.html#step3",
    "href": "posts/HW1/index.html#step3",
    "title": "SQL Queries and Plotly Visualizations",
    "section": "Creating a Data Visualization",
    "text": "Creating a Data Visualization\n\nimport plotly.express as px\nfrom sklearn.linear_model import LinearRegression\n\ndef LR_group_coef(df, x_cols, y_col):\n    '''\n    takes a data frame, runs a linear regression, finds the slope coefficient for the first column in x_cols\n    @ input:\n    - df (df): data frame to perform linear regression on\n    - x_cols (list): list of string column names\n    - y_col (str): response column name\n    @ output:\n    - first slope coefficient\n    '''\n    return LinearRegression().fit(df[x_cols], df[[y_col]]).coef_[0][0]\n\ndef temperature_coefficient_plot(country, year_begin, year_end, month, min_obs, **kwargs):\n    '''\n    plots an interactive geoscatter plot (plotly) with stations as points, with color denoting inferred yearly increase in temperature at that station for that given time period (years) and month.\n    @input:\n    - country (str): (official country name)\n    - year_begin (int): when time period starts\n    - year_end (int): when time period ends\n    - month (int): which month to subset\n    - min_obs (int): minimum number of observations a station must have for this data to run\n    - **kwargs: keyword arguments for plotting\n    @output:\n    - figure: see description\n    '''\n    # get the necessary data\n    df1 = query_climate_database(country, year_begin, year_end, month)\n\n    # drop the stations that have less than n_obs values\n    df2 = df1.copy()\n    df2['n_obs'] = df2.groupby('NAME')['Month'].transform(len)\n    df2['valid'] = df2['n_obs'] >= min_obs\n    df = df2[df2['valid'] == True]\n    df = df.drop(columns = [\"n_obs\", \"valid\"])\n\n    # prep for regression\n    X = df.drop(columns = ['Temp'])\n    y = df[['Temp']]\n\n    # find the average change in temp for each station\n    model_coef = df.groupby('NAME').apply(LR_group_coef,\n    x_cols = ['Year', 'Month'], y_col = 'Temp')\n\n    # map it to the respective stations\n    df['Average Temp Increase'] = np.round(df['NAME'].map(dict(model_coef)), 3)\n\n    # recode months\n    month_recode = {1: 'January',\n    2: 'February',\n    3: 'March',\n    4: 'April',\n    5: 'May',\n    6: 'June',\n    7: 'July',\n    8: 'August',\n    9: 'September',\n    10: 'October',\n    11: 'November',\n    12: 'December'}\n\n    # make the geoscatter plot\n    title = \"Estimates of yearly increase in temperature in {0}<br>for stations in {1}, years {2}-{3}\".format(month_recode[month], country, year_begin, year_end)\n    fig = px.scatter_mapbox(df, lat = 'LATITUDE', lon = 'LONGITUDE',\n    color = 'Average Temp Increase', hover_name = 'NAME', color_continuous_midpoint = 0, title = title, **kwargs)\n\n    fig.show(renderer = \"notebook\")\n    # return df\n\ncolor_map = px.colors.diverging.RdBu_r\n\ntemperature_coefficient_plot(\"India\", 1980, 2020, 1, 10,\n                                    zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\n\n\n                                                \n\n\nHere is another plot for China in the same time period for January.\n\ntemperature_coefficient_plot(\"China\", 1980, 2020, 1, 10,\nzoom = 2,\nmapbox_style = 'carto-positron',\ncolor_continuous_scale = color_map)"
  },
  {
    "objectID": "posts/HW1/index.html#additional-examples",
    "href": "posts/HW1/index.html#additional-examples",
    "title": "SQL Queries and Plotly Visualizations",
    "section": "Additional Examples",
    "text": "Additional Examples\n\ndef monthly_temp(country, year_begin, year_end):\n    '''\n    runs a SQL query that obtains temperature data for a station in a certain country for given years.\n    @input:\n    - country (str): (official country name)\n    - year_begin (int): starting year to look for\n    - year_end (int): end year to look for\n    @output:\n    - df: takes SQL results as a dataframe\n    '''\n\n    # SQL query\n    cmd = '''\n    SELECT s.Name, s.latitude, s.longitude, c.Name, t.year, t.month, t.temp\n    FROM temperatures t\n    LEFT JOIN stations s ON t.id = s.id\n    LEFT JOIN countries c ON SUBSTRING(t.id, 1, 2) = c.[FIPS 10-4]\n    WHERE c.Name == \\\"{0}\\\"\n    AND t.year <= {2} AND t.year >= {1}'''.format(country, year_begin, year_end)\n\n    # establish connection\n    return pd.read_sql_query(cmd, conn) # return data frame\n\ndef median_monthly_temp(country, year_begin, year_end, **kwargs):\n    '''\n    shows the distribution of temperature by month for a station\n    @ input:\n    - country (str): (official country name)\n    - year_begin (int): starting year to look for\n    - year_end (int): end year to look for\n    - **kwargs: keyword arguments for px.box\n    @ output:\n    fig: displays the boxplot\n    '''\n\n    # get necessary data\n    df = monthly_temp(country, 1980, 2020)\n\n    # recode month\n    month_recode = {1: 'January',\n    2: 'February',\n    3: 'March',\n    4: 'April',\n    5: 'May',\n    6: 'June',\n    7: 'July',\n    8: 'August',\n    9: 'September',\n    10: 'October',\n    11: 'November',\n    12: 'December'}\n    df['Month'] = df['Month'].map(month_recode)\n\n    # plot the box plot\n    title = 'Distribution of Temperature by Month,<br>{0} {1}-{2}'.format(country, year_begin, year_end)\n    fig = px.box(df, x=\"Month\", y=\"Temp\", title = title, **kwargs)\n    fig.show(renderer = \"notebook\")\n\nmedian_monthly_temp('India', 1980, 2020)\n\n\n                                                \n\n\nWith this query, we extracted all the necessary information to show a distribution of temperature by month for a given country. In the example plot, we have a distribution of mean temperatures for each month in India. This helps to answer the question: Which month is most likely to have the highest mean temperatures for a country? We can see that through the visualization, it gives us that information, while also showing a general distribution to compare between months easily. This utilizes the box plot to help diminish the issue of missing data.\n\ndef monthly_temp_time(country, year_begin, year_end, station, **kwargs):\n    '''\n    shows the trend of mean temperature by month for a station in a country\n    @ input:\n    - country (str): (official country name)\n    - year_begin (int): starting year to look for\n    - year_end (int): end year to look for\n    - station (str): station name\n    - **kwargs: keyword arguments for px.line\n    @ output:\n    fig: displays the lineplot\n    '''\n    \n    # get necessary data\n    df = monthly_temp(country, year_begin, year_end)\n\n    # sort the dataframe\n    sorted_df = df.copy()\n    # map the value order\n    sorted_df[\"order\"] = sorted_df[\"NAME\"].map({station: 1}).fillna(2)\n    \n    # recode months\n    month_recode = {1: 'January',\n    2: 'February',\n    3: 'March',\n    4: 'April',\n    5: 'May',\n    6: 'June',\n    7: 'July',\n    8: 'August',\n    9: 'September',\n    10: 'October',\n    11: 'November',\n    12: 'December'}\n    sorted_df['Month'] = sorted_df['Month'].map(month_recode)\n    # sort by this order\n    sorted_df.sort_values(by=[\"order\",\"Year\"], ascending=False, inplace=True)\n\n    # make the plot\n    title = \"Progression of Temperature in {0} by Month, {3} {1}-{2}\".format(country, year_begin, year_end, station)\n    fig = px.line(sorted_df, x = 'Year', y = 'Temp', color = 'NAME', **kwargs)\n    fig.update_traces({\"line\":{\"color\":\"lightgrey\"}}) # hide unnecessary data\n    fig.update_traces(patch={\"line\":{\"color\":\"red\", \"width\":3}}, \n                  selector={\"legendgroup\":station}) # highlight specific station\n    fig.update_layout(title=title,\n                showlegend=False,\n                margin = {'l': 0, 'r':0, 't': 50, 'b': 0},\n                width = 800,\n                ) # format\n    fig.show(renderer = \"notebook\")\n\nmonthly_temp_time('India', 1980, 2020, 'TEZPUR', facet_col = 'Month', facet_col_wrap = 4)\n\n\n                                                \n\n\n\nconn.close()\n\nThis plot is created from the same query, but is now looking at trends over time. We ask the question: What trends, if any, are there about the mean temperature by month over the years for a given station? This utilizes a line plot to point out potential trends, and uses selective highlighting to point out a specific station with reference to other stations within the country. It also tells us about potential missing data, since not all lines start at the same point. For this particular station Tezpur, we do not see any particular trend for any given month, but that would have to be verified with numbers."
  },
  {
    "objectID": "posts/HW1/index.html#conclusion",
    "href": "posts/HW1/index.html#conclusion",
    "title": "SQL Queries and Plotly Visualizations",
    "section": "Conclusion",
    "text": "Conclusion\nThrough this post, we learned how to create a database, how to construct and run queries, and to create cool data visualizations on plotly. As you get more familiar with plotly, I would highly recommend you look into the official documentation to better customize your plots to fit your needs and what you need to convey to the audience."
  },
  {
    "objectID": "posts/HW0/index.html",
    "href": "posts/HW0/index.html",
    "title": "HW0",
    "section": "",
    "text": "This post will go through the basics of creating a descriptive data visualization step-by-step. By the end of this post, you will be able to"
  },
  {
    "objectID": "posts/HW0/index.html#preprocessing",
    "href": "posts/HW0/index.html#preprocessing",
    "title": "HW0",
    "section": "Preprocessing",
    "text": "Preprocessing\nFirst, in order to create the visualization, we need the proper data to visualize. For this post, we will be using a dataset regarding penguins, given from a specified website referenced within the code.\n\nimport pandas as pd\nimport numpy as np\nurl = \"https://raw.githubusercontent.com/PhilChodrow/PIC16B/master/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\n\nWhen looking to create a good data visualization, it is essential to look at the data to get a good sense of what you are working with.\n\nprint(penguins.shape)\npenguins.head()\n\n(344, 17)\n\n\n\n\n\n\n  \n    \n      \n      studyName\n      Sample Number\n      Species\n      Region\n      Island\n      Stage\n      Individual ID\n      Clutch Completion\n      Date Egg\n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Sex\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Comments\n    \n  \n  \n    \n      0\n      PAL0708\n      1\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N1A1\n      Yes\n      11/11/07\n      39.1\n      18.7\n      181.0\n      3750.0\n      MALE\n      NaN\n      NaN\n      Not enough blood for isotopes.\n    \n    \n      1\n      PAL0708\n      2\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N1A2\n      Yes\n      11/11/07\n      39.5\n      17.4\n      186.0\n      3800.0\n      FEMALE\n      8.94956\n      -24.69454\n      NaN\n    \n    \n      2\n      PAL0708\n      3\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N2A1\n      Yes\n      11/16/07\n      40.3\n      18.0\n      195.0\n      3250.0\n      FEMALE\n      8.36821\n      -25.33302\n      NaN\n    \n    \n      3\n      PAL0708\n      4\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N2A2\n      Yes\n      11/16/07\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      Adult not sampled.\n    \n    \n      4\n      PAL0708\n      5\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N3A1\n      Yes\n      11/16/07\n      36.7\n      19.3\n      193.0\n      3450.0\n      FEMALE\n      8.76651\n      -25.32426\n      NaN\n    \n  \n\n\n\n\nSince this is not necessarily a data cleaning post, I will not explain too much about what to look out for, but the code will be provided for you to look through.\n\n\nCode\n# change the species column\npenguins['Species'] = penguins['Species'].str.split().str.get(0)\n# drop NAs corresponding to culmen length or depth\np_cleaned = penguins.dropna(subset=['Culmen Length (mm)', 'Culmen Depth (mm)'])\n\np_cleaned = p_cleaned[p_cleaned.Sex != '.'] # other data cleaning\n\n# subset by useful data\npenguins = p_cleaned[['Species',\n'Culmen Length (mm)',\n   'Culmen Depth (mm)',\n   'Flipper Length (mm)',\n   'Body Mass (g)',\n   'Delta 15 N (o/oo)',\n   'Delta 13 C (o/oo)']]\n\n\nOnce you have clean data, you can move on to the visualization process."
  },
  {
    "objectID": "posts/HW0/index.html#visualization",
    "href": "posts/HW0/index.html#visualization",
    "title": "HW0",
    "section": "Visualization",
    "text": "Visualization\n\n1. Decide on what data you want to visualize\nIt may be helpful to look at some bivariate summary statistics to help gauge any particular interest with specific variables. This will avoid unnecessary time looking at uninformative visualizations.\n\n# creates summary statistics of numeric variables by species\npenguins.groupby(['Species']).aggregate([np.mean, np.std])\n\n\n\n\n\n  \n    \n      \n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n    \n    \n      \n      mean\n      std\n      mean\n      std\n      mean\n      std\n      mean\n      std\n      mean\n      std\n      mean\n      std\n    \n    \n      Species\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Adelie\n      38.791391\n      2.663405\n      18.346358\n      1.216650\n      189.953642\n      6.539457\n      3700.662252\n      458.566126\n      8.859733\n      0.426217\n      -25.804194\n      0.588186\n    \n    \n      Chinstrap\n      48.833824\n      3.339256\n      18.420588\n      1.135395\n      195.823529\n      7.131894\n      3733.088235\n      384.335081\n      9.356155\n      0.368720\n      -24.546542\n      0.238809\n    \n    \n      Gentoo\n      47.529508\n      3.082386\n      14.976230\n      0.983084\n      217.188525\n      6.511696\n      5077.663934\n      505.862403\n      8.247026\n      0.264909\n      -26.185305\n      0.540793\n    \n  \n\n\n\n\nWithin this table, I look for any distinct differences that might differentiate certain groups of penguins from others. This can be done with any type of grouping (with Island and any other categorical variable).\nWe see that within each sex, there seems to be a clear distinction with Adelie penguins from the other two species with regard to Culmen Length. We also see this similar distinction with Gentoo penguins and the other two with regards to Culmen Depth.\nI would encourage you to form more tables if you would like to find more insights, but for the purposes of this demonstration, this is sufficient information to create a visualization based on Culmen Length and Culmen Depth.\n\n\n2. What is the best type of visualization (for your data)?\nObjectively, the best visualization is a pie chart (just kidding). Each type of visualization serves its own purpose given the type of data you want to visualize. Here are some pointers for which visualization to use for different data.\n\nScatter plots\nScatter plots are most useful with two continuous variables, as it seeks to plot points within a 2-dimensional grid. This will match what we want to visualize with Culmen Length (mm) and Culmen Depth (mm).\n\n\nBar plots, box plots\nBoth bar and box plots are most useful when you are graphing a continuous variable with a categorical variable. One of the axis will denote the categories, while the other will display the length (or spread) corresponding to the continuous variable.\n\n\nTables\nThese are helpful with categorical data, seeing if there is any correlation, or displays the relative frequency of the data.\n\n\nHistograms\nThis is helpful in a univariate and bivariate setting. Both display the data in sets of bins that will display the distribution of data among those points. The data can be discrete or continuous, but the axes will always be coerced to categorical bins.\nThere are a bunch of different plot types, which I will not go more in depth about, but please explore on the official documentation of matplotlib, plotly or any other plotting software that you might use.\n\n\n\n3. Create your visualization\nFor this example, we will use matplotlib to create my visualization. As stated above, we will use a scatter plot to display this information. The necessary parameters for a scatter plot are the data that will into the x and the y axis.\n\nfrom matplotlib import pyplot as plt\n\n# create a scatter plot by culmen length and culmen depth\nfig, ax = plt.subplots(1)\nax.scatter(penguins['Culmen Length (mm)'], penguins['Culmen Depth (mm)'])\nplt.show()\n\n\n\n\nThis fulfills the requirement of plotting. However, if someone was given this visualization, it would be completely uninformative. What are the axes measuring? In what unit? What does each point represent? We need to add some informative labels and narrative text that best describes this.\n\n\n4. Label the visualization\n\n# create a scatter plot by culmen length and culmen depth\nfig, ax = plt.subplots(1)\nax.scatter(penguins['Culmen Length (mm)'], penguins['Culmen Depth (mm)'])\n\n# add labels\nax.set(xlabel = 'Culmen Length (mm)', ylabel = 'Culmen Depth (mm)',\ntitle = \"Relationship between Penguin Culmen Depth and Length\")\nplt.show()\n\n\n\n\nNice! We have some points on a grid that represent Culmen Length and Culmen Depth. What does this visualization tell us about the relationship between the two variables? At a first glance, it seems like there is no real indicator of any correlation between the two. Recall the information that we learned about from the summary statistics. There were differences seen when separated by the Species. Let us implement the first through some color contrast, which can be done through adding parameters to the plt.scatter() function.\n\n\n5. Subset the data by groups\n\n# create a scatter plot by culmen length and culmen depth\nfig, ax = plt.subplots(1)\n# for each species, map out the culmen length/depth of each penguin\nfor speci in penguins['Species'].unique():\n    ax.scatter(penguins['Culmen Length (mm)'][penguins['Species'] == speci],\n     penguins['Culmen Depth (mm)'][penguins['Species'] == speci])\n\n# add labels\nax.set(xlabel = 'Culmen Length (mm)', ylabel = 'Culmen Depth (mm)',\ntitle = \"Relationship between Penguin Culmen Depth and Length\")\nplt.show()\n\n\n\n\nHow do we know which color corresponds to each species? We now need to add a legend.\n\n# create a scatter plot by culmen length and culmen depth\nfig, ax = plt.subplots(1)\n# for each species, map out the culmen length/depth of each penguin\nfor speci in penguins['Species'].unique():\n    ax.scatter(penguins['Culmen Length (mm)'][penguins['Species'] == speci],\n     penguins['Culmen Depth (mm)'][penguins['Species'] == speci],\n     label = speci) # added label to specify each species\n\n# add labels\nax.set(xlabel = 'Culmen Length (mm)', ylabel = 'Culmen Depth (mm)',\ntitle = \"Relationship between Penguin Culmen Depth and Length\")\n\n# adds a legend, which automatically is displayed if \"label\" is set in ax.scatter\nax.legend()\nplt.show()\n\n\n\n\n\n\n6. What insights can you draw from the visualization?\nGreat! You have now created your first data visualization! However, a pretty visualization means nothing unless you can draw insight out of it. How does this visualization help further your analysis? How would the readers interpret this graphic?\nIn this example, we have visualized the relationship between Culmen Length and Culmen Depth for penguins by species. The clustering within each group suggests that there may be some boundaries (with error) that could accurately assess the species of a penguin through these two traits. This can lead down the path of creating more visualizations to be exhaustive with your search, or to perhaps create a model."
  },
  {
    "objectID": "posts/HW0/index.html#conclusion",
    "href": "posts/HW0/index.html#conclusion",
    "title": "HW0",
    "section": "Conclusion",
    "text": "Conclusion\nThrough this post, you have learned how to create a visualization. I would highly encourage you to explore more of this Palmer penguins data set. Experiment with different columns and plot types to see what other insights you can extract and share with the world!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Joshua’s Blog",
    "section": "",
    "text": "9/10 Journalisms Hate This Model\n\n\n\n\n\n\n\n\n\n\n\n\nMar 10, 2023\n\n\nJoshua Li\n\n\n\n\n\n\n  \n\n\n\n\nClassifying Pets through Convolutional Neural Networks with Tensorflow\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 28, 2023\n\n\nJoshua Li\n\n\n\n\n\n\n  \n\n\n\n\nCreating a Message Bank with Flask\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 17, 2023\n\n\nJoshua Li\n\n\n\n\n\n\n  \n\n\n\n\nScraping TMDB with Scrapy\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 8, 2023\n\n\nJoshua Li\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSQL Queries and Plotly Visualizations\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nFeb 1, 2023\n\n\nJoshua Li\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW0\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJan 23, 2023\n\n\nJoshua Li\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nJan 10, 2023\n\n\nJoshua Li\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi everyone! I’m Josh, an aspiring Data Scientist studying at UCLA, graduating March 2023. My programming experience is split extensively between R and Python, with some experience in PostgreSQL. My other links are referenced below, but this blog consists of my project write-ups, including code. Please look around!"
  }
]