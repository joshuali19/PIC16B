{"title":"Scraping TMDB with Scrapy","markdown":{"yaml":{"title":"Scraping TMDB with Scrapy","author":"Joshua Li","date":"2023-02-08","jupyter":"pic16b"},"headingText":"Extract elements from a web page","headingAttr":{"id":"step1","classes":[],"keyvalue":[]},"containsRefs":false,"markdown":"\n\nFor this blog post, we will dive into web scraping. We will be looking into the movie database (TMDB) as our website which we will scrape data from. **The goal of this article will be to look at a specific TV show, find all of its actors, and record their whole filmography (as actors).** After reading through this article, you will be able to:\n\n1. [Extract elements from a web page](#step1)\n2. [Create a spider](#step2)\n3. [Automate extraction through the spider](#step3)\n4. [Figure out how to avoid website blockers](#step4)\n\n\nThis is practice to figure out what CSS elements are needed to be extracted. This is done manually so that you get a good sense of how to access specific elements. In this example, we will be looking at the web page corresponding to [*Community*](https://www.themoviedb.org/tv/18347-community). \n\nIn general, the framework of extracting elements is as follows:\n- find the element you want to extract\n- right-click on the element, and select \"Inspect\".\n- note the HTML tags that uniquely identify that element.\n\n### Navigate to \"Full Cast & Crew\"\n\nFor this project, we want to view the \"Full Cast & Crew\" page of the TV page. We will find that in the middle-left of the page, and we can see that the HTML tags wrapped around it are `<section class=\"panel top_billed\">`, `<p>` and `<a>` tags. We can see the URL we want to go to reflects a relative path that adds to the current web page (i.e. `/cast`).\n\n![](HTML_code8.png)\n\nUnderlying HTML Code:\n![](HTML_code2.png)\n\nWe will now go to this page.\n\n### Navigate to an Actor's Page\n\nSince we want to find the filmography of every actor, we need to navigate to each individual actor's page. For this example, we will look at navigating to Joel McHale's (plays Jeff Winger in *Community*) page.\n\nThe element we want is McHale's headshot, which provides a URL to go to his page. We inspect this element, and see that his headshot is within a `<ol class=\"people credits\">`, `<li>`, `<a>` tag. We want the URL found in the `<a>` tag, in the attribute `href`. \n\n![](HTML_code9.png)\n\nUnderlying HTML Code:\n![](HTML_code1.png)\n\nWe can now navigate to his page.\n\n### Extract all the works they have acted in\n\nNow, we are able to look at McHale's work in the `Acting` section. We will look at how to extract his name from the page, as well as \"California King\" from the filmography. \n\nFirst, we will extract his name from the big title on the top of the page. We can see that the element is wrapped in a `<div class=\"title\">`, and `<a>` tag.\n\n![](HTML_code3.png)\n\nUnderlying HTML Code:\n![](HTML_code4.png)\n\nNow, let us look at \"California King\". We can see that it is wrapped in a `<table class=\"card credits\">`, `<a class=\"tooltip\">`, and `<bdi>` tag.\n\n![](HTML_code3.png)\n\nUnderlying HTML Code:\n![](HTML_code4.png)\n\nWith this information, we can now begin to automate this process with `scrapy`.\n\n## Create A Spider {#step2}\n\nThis section will look at the creation of a spider. There are a couple of steps that you need to take.\n\n1. Create a new repository\n2. Ensure `scrapy` is in your environment \n3. Start a `scrapy` project in the repository\n\n### Create a new repository\n\nThe first step is to create a new GitHub repository. This is done through a [GitHub](https://github.com/) account. This repository will house your spider, and can be version controlled to account for different changes. Sync it up with your corresponding GitHub Desktop account, and clone the repository to your local device with this [tutorial](https://git-scm.com/book/en/v2/Git-Basics-Getting-a-Git-Repository).\n\nNow that the repository is local, we can move on to the next step.\n\n### Ensure `scrapy` is in your environment\n\nThe easiest way to do this is to open up **Anaconda-Navigator**, and to look at the tab \"Environments\". Find the environment that corresponds to the one you will run the spider on. Search for `scrapy` on the installed packages. If it doesn't show up, find it on the uninstalled packages and install it.\n\n### Start a `scrapy` project in the repository\n\nSince you have `scrapy` installed and a local repository, navigate to the local repository via Terminal, and type the following:\n\n```\nconda activate <myenv>\nscrapy startproject TMDB_scraper\ncd TMDB_scraper\n```\n\nThis will automatically create a spider with the necessary python files for it to run. In particular, we will use `settings.py`. Go into the `spiders` folder and also create a `tmdb_spider.py` file and write this as the base.\n\n```{python}\n# to run \n# scrapy crawl tmdb_spider -o movies.csv\n\nimport scrapy\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n    \n    start_urls = ['https://www.themoviedb.org/tv/18347-community']\n```\n\n## Automate extraction through the spider {#step3}\n\nWe will now automate extraction through the spider. This is where we will need to recall the information we gained earlier (see [above]{#step1}), as well as have knowledge of CSS selectors. Find out more about CSS selectors [here](https://flukeout.github.io/) and [here](https://www.w3schools.com/cssref/css_selectors.php). As you experiment with how to extract this via code, it is helpful to use [`scrapy shell`](https://docs.scrapy.org/en/latest/topics/shell.html) instead of using trial and error and running `.py` files. \n\n### Scrapy Shell\n\nHere is a small example of using `scrapy shell` for experimentation. Navigate to the repository, open up a terminal, and run `scrapy shell https://www.themoviedb.org/tv/18347-community`(**NOTE**: You may run into errors such as 403, please look to [here](#step4) for guidance on how to resolve that; you want to see numbers of the form 200).\n\nHere is what happens when you run `scrapy shell https://www.themoviedb.org/tv/18347-community`, which is a request to access the main page for *Community*.\n\n![](HTML_code7.png)\n\nYou can see that there is are two lines of `GET (200) ...`, which signifies that the request went through. I then sent a request to find the URL for the \"Full Cast & Crew\" page, by subsetting via the tags it is wrapped in:\n```\nresponse.css(\"section.top_billed p.new_button a\").attrib['href']\n```\n\nThis can be done to experiment with other subsetting to ensure you are getting the right elements.\n\n### Creating Functions to Scrape TMDB\n\nNow, we are able to create a web scraper, complete with functions that will do it for us at one command. There are three main functions necessary for us to scrape the filmography of actors from *Community*. We need a function to parse to main TV page, one to parse the list of actors, and one to parse the actor's filmography. All of these functions should go into the `tmdb_spider.py` file under the `TmdbSpider` class. I will go through each one.\n\n#### Repository\n\nHere is the [repository](https://github.com/joshuali19/tmdb) that houses all the data and code for this scraper.\n\n#### Parsing main TV page\n\n```{python}\ndef parse(self, response):\n        '''\n        Parses the TMDB cast and crew website.\n        @ input:\n        - self: TmdbSpider\n        - response: the call to start_urls[0]\n        @ output:\n        - yields a request to navigate to the page with actors.\n        '''\n        cast = self.start_urls[0] + '/cast' # hardcode the cast page\n        \n        # go to cast & crew page, run parse_full_credits\n        yield scrapy.Request(cast, callback = self.parse_full_credits)\n```\n\nThis function gets the starting URL, which is linked to the TMDB *Community* page. Since we recognize that the \"Full Cast & Crew\" can be found by just adding `/cast` to the end of the URL, we just hardcode that in, and send a request to go to that URL, and to run `parse_full_credits`.\n\n#### Parsing the acting credits page\n```{python}\ndef parse_full_credits(self, response):\n        '''\n        for each actor, goes to their respective acting profile page on TMDB.\n        @ input:\n        - self: TmdbSpider\n        - response: the call to the \"Full Cast & Crew\" page\n        @ output:\n        - yields a request to navigate to the profile page of each actor.\n        '''\n        \n        # for each page redirection on the cast photos\n        for page in response.css('ol.people.credits:not(.crew) li a'):\n            actor_page = page.attrib['href'] # obtain the hyperlink\n            actor_page = 'https://www.themoviedb.org' + actor_page # append to main url\n            \n            # go to the actor's page, run parse_actor_page\n            yield scrapy.Request(actor_page, callback = self.parse_actor_page)\n```\n\nThis function assumes that we are on the page with all the actors in *Community*. We now need to navigate to every actor's home page. As mentioned earlier, we know that the link is contained within the HTML tags of `<ol class=\"people credits\">`, `<li>`, and `<a>`. However, that alone is not enough to get **ONLY** the actors; it includes the crew members. We don't want that, so looking further into the HTML code, we see that the crew members has a special designation in their tag `<ol class=\"people credits crew\">`. We can use this to our advantage to omit any `<ol>` tags with class \"crew\" with the `:not()` CSS selector. Now, we get that for every element within the actors section, obtain the hyperlink for their page, and submit a request to go to the actor's profile for every actor in *Community*, and run `parse_actor_page`.\n\n#### Parsing actor's profile page\n```{python}\ndef parse_actor_page(self, response):\n        '''\n        obtains the films of the actor.\n        @ input:\n        - self: TmdbSpider\n        - response: the call to the actor's page\n        @ output:\n        - yields a dictionary with actor name and movie.\n        '''\n        # obtain the actor name\n        actor_name = response.css('div.title a::text').get()\n        \n        # for each of the links in the acting section of his or her page\n        for acting_gig in response.css('h3.zero + table.card.credits a.tooltip bdi::text'):\n            title = acting_gig.get() # obtain the right URL\n            \n            yield {'actor': actor_name, 'movie_or_TV_name': title} \n            # yield a dictionary with actor and title of movie they were in.\n```\n\nThis function assumes that you are on the actor's page. We first need to get the actor's name for our dictionary, which is conveniently placed in bold font at the top of the page in a `<div class=\"title\">` and `<a>` tag. The harder part is getting the movies/TV shows that the actor has acted in. In this case, it is found underneath `<h3 class=\"zero\">`. The selectors to the right of the `+` operator denote that you only want to find elements within `h3.zero`. You go within the `<table class=\"card credits\">` and `<a class=\"tooltip\">` and `<bdi>` tags to get the text within these subsets. That will give you the title. For every film, we can yield a dictionary linking a person with a specific film.\n\nWith these three functions, we are able to run the spider and have it crawl the TMDB website for *Community* websites. This can apply to any other start URL for other movies or TV shows.\n\n## Figure out how to avoid blockers {#step4}\n\nThis is something that will be common as you scrape the web more. Websites will have methods that will determine if there is a bot or a program scraping data. If they detect suspicious behavior of this sort, they will block the request to obtain data, which will be displayed in the form of `40_` errors. This is a hinderance to data collection, but there are workarounds for it. \n\nFor this example with the default settings, TMDB will block requests to access their data, creating a `403` error. My workaround was based off of this [article](https://scrapeops.io/python-scrapy-playbook/scrapy-403-unhandled-forbidden-error/): I changed three lines in my `settings.py` file. \n\nBy default, your `scrapy` user agent will identify itself as a scraper when it goes to scrape websites. However, this is a clear indicator for websites to block the request. I changed the user agent by assigning a common user agent in my `settings.py` file.\n\n```{python}\n## settings.py\n\nUSER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.79 Safari/537.36 Edge/14.14393'\n```\n\nAnother way that websites can discern a suspicious request is the speed of download requests. An automated bot that knows where to look would be faster than any human interaction with the interface. Therefore, it is important to set a good delay of download to not raise suspicions.\n\n```{python}\n## settings.py\n\n# Configure a delay for requests for the same website (default: 0)\n# See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay\n# See also autothrottle settings and docs\nDOWNLOAD_DELAY = 5\n```\n\nThe last thing I did was disable cookies, because some websites use cookies to spot bot behavior.\n\n```{python}\n## settings.py\n\n# Disable cookies (enabled by default)\nCOOKIES_ENABLED = False\n```\n\nHowever, these changes may not work for everyone. I would advise you to check out these other links ([link1](https://doc.scrapy.org/en/latest/topics/practices.html#avoiding-getting-banned), [link2](https://scrapeops.io/web-scraping-playbook/403-forbidden-error-web-scraping/), [link3](https://scrapingrobot.com/blog/most-common-user-agents/)) for more help.\n\n## Bonus: Visualizing Scraped Data \n\nWith this scraper, we are able to save our findings in a `.csv` file with the actor and respective movies they acted in. This is done through the terminal, running the command\n\n```\nscrapy crawl tmdb_spider -O movies.csv\n```\n\n\nThis command is telling the name of our spider `tmdb_spider` to scrape the web, and to overwrite output (`-O`) on a relative path to a file `movies.csv`. Replace `-O` with `-o` if you want to append the output to the current file instead. Now, you should be able to have the data accessible in `movies.csv` for you to use. We can use this data to create visualizations for the most common movies/TV shows that these actors have also played in. \n\n```{python}\nimport pandas as pd\nimport numpy as np\n\nmovies = pd.read_csv(\"./movies.csv\")\n```\n\n```{python}\nmovies\n```\n\n```{python}\n# get the count of actors in each TV/movie\nshared_movies = movies.groupby('movie_or_TV_name').agg(len).sort_values('actor', ascending = False).reset_index()\n```\n\n```{python}\nt20_shared_movies = shared_movies[1:21] # gets the first 20 films, besides Community (#1)\n```\n\n```{python}\nimport plotly.express as px\n\nfig = px.bar(t20_shared_movies,\n            x = 'movie_or_TV_name',\n            y = 'actor',\n            labels = {'movie_or_TV_name': 'Movie/TV Show',\n                     'actor': 'Number of Shared Actors'},\n            title = \"Top 20 Shows with Shared Actors with Community Cast\",\n            height = 700,\n            text_auto = '.2s')\nfig.show(renderer = 'notebook')\n#fig.show()\n```\n\nThis ranking of shows with shared actors is interesting. Coming into this post, I expected that there would be at least some Marvel movie, given that *Community* is directed by the Russo Brothers, and a majority of the cast has appeared on some of their movies. Instead, we are given many different TV shows. This makes sense given that people have the potential to make guest appearances to most of these shows. Likewise, *Community* itself is a show with many guest appearances due to their many homages to popular culture and movie tropes. A future direction would be to select only the actors who appear in a \"majority\" of episodes, and to only include recurring characters (not one episode guest appearances), to see if this list would change. However, we can see from this that the whole cast of Community has a wide array of actors with popular TV shows on their filmography.\n\n## Conclusion\n\nNow, you should be able to get started with web scraping! Take your time in learning CSS selectors, and finding the right tags to be your parameters. Be patient if it takes a while to fetch your requests, especially if websites block it. There is a workaround for most websites. Test it out for yourself and see the plethora of data that becomes available to you!"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"index.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.313","theme":"flatly","title-block-banner":true,"title":"Scraping TMDB with Scrapy","author":"Joshua Li","date":"2023-02-08","jupyter":"pic16b"},"extensions":{"book":{"multiFile":true}}}}}