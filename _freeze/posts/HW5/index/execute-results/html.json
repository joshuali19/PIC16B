{
  "hash": "7ccfb84b399939061fcf8b4fa18faa91",
  "result": {
    "markdown": "---\ntitle: 9/10 Journalisms Hate This Model\nauthor: Joshua Li\ndate: '2023-02-28'\n---\n\n# Classifying Fake News with TensorFlow\n\nThe purpose of this blog post is to dive deeper into the functionality of `TensorFlow` in building neural networks. This time, we will be looking into classifying articles as fake news depending on the article text, the title, or both. \n\n## Data Source\n\nLet us first get the data necessary for the project. This dataset is originally from a research article:\n\n- Ahmed H, Traore I, Saad S. (2017) “Detection of Online Fake News Using N-Gram Analysis and Machine Learning Techniques. In: Traore I., Woungang I., Awad A. (eds) Intelligent, Secure, and Dependable Systems in Distributed and Cloud Environments. ISDDC 2017. Lecture Notes in Computer Science, vol 10618. Springer, Cham (pp. 127-138).\n\nThey have publicized their data on [Kaggle](https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset).\n\nAnother person (Phil Chodrow) has taken the liberty to process the data and host the data set in a URL for us to readily access. Below is the code to load in the necessary data, along with all the necessary packages we need for this project. \n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\n```\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# load data\ntrain_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true\"\nnews_df = pd.read_csv(train_url)\nnews_df.columns = ['id', 'title', 'text', 'fake']\n```\n:::\n\n\n::: {.cell outputId='b2eb68b2-5945-4216-b437-4c927c4cac2b' execution_count=3}\n``` {.python .cell-code}\nnews_df.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>title</th>\n      <th>text</th>\n      <th>fake</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>17366</td>\n      <td>Merkel: Strong result for Austria's FPO 'big c...</td>\n      <td>German Chancellor Angela Merkel said on Monday...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5634</td>\n      <td>Trump says Pence will lead voter fraud panel</td>\n      <td>WEST PALM BEACH, Fla.President Donald Trump sa...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>17487</td>\n      <td>JUST IN: SUSPECTED LEAKER and “Close Confidant...</td>\n      <td>On December 5, 2017, Circa s Sara Carter warne...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>12217</td>\n      <td>Thyssenkrupp has offered help to Argentina ove...</td>\n      <td>Germany s Thyssenkrupp, has offered assistance...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5535</td>\n      <td>Trump say appeals court decision on travel ban...</td>\n      <td>President Donald Trump on Thursday called the ...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nThe data frame contains 4 columns: `id`, `title`, `text`, and `fake`. Here is what each variable represents:\n\n- `id` (int): A unique identifier for the article\n- `title` (str): Title of the article\n- `text` (str): full body text of the article\n- `fake` (bool): whether or not the article is fake news (true label)\n\n## Processing the Data\n\nEven though this dataset is cleaned up well, there are some more preprocessing steps that are needed before we can model the data. Since we are analyzing text data, we need to do two things:\n\n1. Remove *stopwords* from the article `text` and `title`. A stopword is classified as any word that is common and uninformative for analysis, such as \"and\", \"but\", \"or\", \"the\". \n2. Transform the dataset into one that can be read by `TensorFlow`, which is done through `tf.data.Dataset`.\n\nWe will create a function to automate these two processes.\n\n::: {.cell outputId='d5706d34-798a-4c45-ed89-535806634e5e' execution_count=4}\n``` {.python .cell-code}\nimport nltk\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nnltk.download('stopwords')\n\nfrom nltk.corpus import stopwords\n\ndef make_dataset(df, text_cols, response_var):\n  '''\n  creates a cleaned tensorflow dataset from a raw dataset.\n  @ inputs:\n  - df (pd.DataFrame): dataFrame containing columns of text data\n  - text_cols (str): column name(s) that represents text\n  - response_var (str): column name that represents the response variable\n  @ outputs:\n  - tfdf (tf.data.Dataset): output dataset with two inputs ((title, text)) and one output (fake).\n  '''\n  # gets stopwords from nltk.corpus library\n  stop = stopwords.words('english')\n\n  new_textcols = []\n  for col in text_cols:\n    new_colname = col + '_wo_stopwords'\n    new_textcols.append(new_colname) # new column name\n    \n    # removes stopwords from text, joins it all together again.\n    df[new_colname] = df[col].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n  \n  # creates a tensor dataframe from the processed text columns as input, and response var as output\n  tfdf = tf.data.Dataset.from_tensor_slices((\n      {\n          \"title\": df[[new_textcols[0]]],\n          \"text\": df[[new_textcols[1]]]\n      }, {\n          \"fake\": df[['fake']]\n      }\n      ))\n  \n  # speeds up performance\n  tfdf = tfdf.batch(100)\n  return tfdf\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n2023-03-10 22:39:27.631067: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/joshuali/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n```\n:::\n:::\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nnews_data = make_dataset(news_df, ['title', 'text'], 'fake')\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n2023-03-10 22:39:55.202223: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n```\n:::\n:::\n\n\n### Creating Validation Dataset\n\nNow that we have the data in a compatible format, we can now take this training data, and split 20% of it for validation purposes in hopes to help improve performance on the unseen data.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n# shuffles the data to ensure no ordering happens with splitting training and validation\nnews_data = news_data.shuffle(buffer_size = len(news_data))\n```\n:::\n\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n# 80% training data, 20% validation data\n\ntrain_size = int(0.8*len(news_data)) \nval_size = int(0.2*len(news_data))\n\ntrain = news_data.take(train_size) # data[:train_size]\nval = news_data.skip(train_size).take(val_size) # data[train_size : train_size + val_size]\n```\n:::\n\n\n### Base Rate\n\nNow, let us look at the base rate that we would like to hit in order to determine whether our model performance is poor or excellent. If our model just guesses one label (e.g. 'fake'), the proportion will be the base rate. We want to do better than this.\n\n::: {.cell outputId='4866e7fd-4612-433d-c573-76f0d57d7380' execution_count=8}\n``` {.python .cell-code}\nfor x, y in train.take(1):\n  print(y['fake'].numpy().sum())\n  print(len(y['fake'].numpy()))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n66\n100\n```\n:::\n:::\n\n\n::: {.cell outputId='981fd971-a31a-4b89-fe60-0cb0802e649c' execution_count=9}\n``` {.python .cell-code}\nfakes = 0\ntotal = 0\n\n# for inputs (x) and output (y) in train\nfor x, y in train:\n  # get number of fakes and total\n  fakes += y['fake'].numpy().sum()\n  total += len(y['fake'].numpy())\n# get baseline rate\nprint((fakes/total))\nprint(total)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.5248203242520475\n17949\n```\n:::\n:::\n\n\nWe can see that the base rate is around 52%. We need to get above this percent accuracy so that our model performs better than just guessing 'fake' all the time.\n\n### Text Vectorization\n\nIn addition to this, we also need to make sure that these words are able to be rendered into the model. The neural network model cannot evaluate strings, so we have to vectorize each word to map to an integer. Here is one of the examples that you can use for the data.\n\n::: {.cell outputId='fce8b782-4cb8-4ab2-9891-47c68c171948' execution_count=10}\n``` {.python .cell-code}\nimport re\nimport string\n#preparing a text vectorization layer for tf model\nsize_vocabulary = 2000\n\ndef standardization(input_data): # word2vec\n    lowercase = tf.strings.lower(input_data)\n    no_punctuation = tf.strings.regex_replace(lowercase,\n                                  '[%s]' % re.escape(string.punctuation),'')\n    return no_punctuation \n\n# vectorize layer\ntitle_vectorize_layer = layers.TextVectorization(\n    standardize=standardization,\n    max_tokens=size_vocabulary, # only consider this many words\n    output_mode='int',\n    output_sequence_length=500) \n\n# adapts to the title words\ntitle_vectorize_layer.adapt(train.map(lambda x, y: x['title']))\n```\n:::\n\n\n## Creating Models\n\nWe will use this processed informations and layers to help create models to classify the integrity of the news article. We will create three different models:\n\n1. Using only **the article title** as an input.\n2. Using only **the article text** as an input.\n3. Using both the **the article title and text** as input.\n\n### 1. Article Title Only\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\n# base input that takes in Article Title\ntitle_input = keras.Input(\n    shape=(1,),\n    name = \"title\", # same name as the dictionary key in the dataset\n    dtype = \"string\"\n)\n```\n:::\n\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\ntitle_features = title_vectorize_layer(title_input) # apply this \"function TextVectorization layer\" to lyrics_input\ntitle_features = layers.Embedding(size_vocabulary, output_dim = 3, name=\"title_embedding\")(title_features) # embeds words\ntitle_features = layers.Dropout(0.2)(title_features) # prevent overfit\ntitle_features = layers.GlobalAveragePooling1D()(title_features)\ntitle_features = layers.Dropout(0.2)(title_features) # prevent overfit\ntitle_features = layers.Dense(32, activation='relu')(title_features) # merge\n\n# x -> f(x) -> f1(f(x)) -> ... -> fn(...f(x))\n```\n:::\n\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\noutput = layers.Dense(1, name=\"fake\")(title_features) # output of 1 since binary\n```\n:::\n\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\n# title model\ntitle_model = keras.Model(\n    inputs = [title_input],\n    outputs = output\n)\n```\n:::\n\n\n::: {.cell outputId='407aee40-06f8-4378-c604-0d0e31bf78a6' execution_count=15}\n``` {.python .cell-code}\ntitle_model.summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"model\"\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n_________________________________________________________________\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n Layer (type)                Output Shape              Param #   \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n=================================================================\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n title (InputLayer)          [(None, 1)]               0         \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                 \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n text_vectorization (TextVec  (None, 500)              0         \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n torization)                                                     \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                 \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n title_embedding (Embedding)  (None, 500, 3)           6000      \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                 \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n dropout (Dropout)           (None, 500, 3)            0         \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                 \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n global_average_pooling1d (G  (None, 3)                0         \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n lobalAveragePooling1D)                                          \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                 \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n dropout_1 (Dropout)         (None, 3)                 0         \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                 \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n dense (Dense)               (None, 32)                128       \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                 \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n fake (Dense)                (None, 1)                 33        \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                 \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n=================================================================\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nTotal params: 6,161\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nTrainable params: 6,161\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nNon-trainable params: 0\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n_________________________________________________________________\n```\n:::\n:::\n\n\n::: {.cell outputId='6ce4ee5d-8349-40ff-e0f4-5dc6ebbba38f' execution_count=16}\n``` {.python .cell-code}\nkeras.utils.plot_model(title_model) # model architecture\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nYou must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n```\n:::\n:::\n\n\n![Title Model Architecture](title_model.png)\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\ntitle_model.compile(optimizer=\"adam\",\n              loss = keras.losses.BinaryCrossentropy(from_logits=True), # binary classification\n              metrics=[\"accuracy\"])\n```\n:::\n\n\n```python\n#| id: hSp-ID4re9BV\n#| colab: {base_uri: 'https://localhost:8080/'}\n#| outputId: e3a4d60b-a850-4ca8-fa73-8a0fadc95b8d\ntitle_history = title_model.fit(train, \n                    validation_data=val,\n                    epochs = 50, \n                    verbose = False)\n```\n\n\n```python\n#| id: fBdT6LJze-Ww\n#| colab: {base_uri: 'https://localhost:8080/', height: 609}\n#| outputId: fdb6cc9c-5420-4d03-e8b1-cd3c00400366\nfrom matplotlib import pyplot as plt\ndef plot_val(history, threshold, figsize = (10, 10), title = 'Model Performance', fontsize = 16):\n  '''\n  plots the training and validation accuracy of a model\n  @ inputs:\n  - history: gets the log of all metrics recorded by epoch in the neural net model\n  - threshold (float): the accuracy to surpass\n  - figsize (tuple): len 2 tuple to denote dimensions of plot\n  - title (str): Title of plot\n  - fontsize (float): font size of title\n  @ outputs:\n  - shows plot of training and validation accuracy by epoch\n  '''\n  fig, ax = plt.subplots(1, figsize = figsize)\n\n  # plots accuracies as lines\n  ax.plot(history.history[\"accuracy\"], label = 'Training Accuracy')\n  ax.plot(history.history[\"val_accuracy\"], label = 'Validation Accuracy')\n\n  # marks the threshold accuracy\n  ax.axhline(y = threshold, c = 'red', ls = '--')\n\n  # Customization\n  ax.legend()\n  ax.set_title(title, fontsize = fontsize)\n  fig.show()\n\nplot_val(title_history, 0.97, title = 'Model Performance with Title Input')\n```\n![Plot1](plot1.png)\n\nWith the titles as the only input, the model performs extremely well in classifying the news article as fake or real news. The red line denotes the 97% accuracy line, which both the training and validation accuracy surpass over a number of epochs. There is no evidence of overfitting, as the validation accuracy consistently surpasses that of the training accuracy by a small margin.\n\n### 2. Article Text Only\n\nNow, we are going to look at a model that only takes text as the input. The process will be very similar as to the first model, but instead of inputting the `title` variable, we will use the `text` variable.\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\n# base text input\ntext_input = keras.Input(\n    shape=(1,),\n    name = \"text\", # same name as the dictionary key in the dataset\n    dtype = \"string\"\n)\n```\n:::\n\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\ntext_vectorize_layer = layers.TextVectorization(\n    standardize=standardization,\n    max_tokens=size_vocabulary, # only consider this many words\n    output_mode='int',\n    output_sequence_length=500) \n\n# adapts vectorize layer to text words\ntext_vectorize_layer.adapt(train.map(lambda x, y: x['text']))\n```\n:::\n\n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\ntext_features = text_vectorize_layer(text_input) # apply this \"function TextVectorization layer\" to lyrics_input\ntext_features = layers.Embedding(size_vocabulary, output_dim = 3, name=\"text_embedding\")(text_features) # embed words\ntext_features = layers.Dropout(0.2)(text_features) # prevent overfitting\ntext_features = layers.GlobalAveragePooling1D()(text_features)\ntext_features = layers.Dropout(0.2)(text_features) # prevent overfitting\ntext_features = layers.Dense(32, activation='relu')(text_features)\n```\n:::\n\n\n::: {.cell execution_count=21}\n``` {.python .cell-code}\noutput = layers.Dense(1, name=\"fake\")(text_features) # binary classification\n```\n:::\n\n\n::: {.cell execution_count=22}\n``` {.python .cell-code}\n# text model\ntext_model = keras.Model(\n    inputs = [text_input],\n    outputs = output\n)\n```\n:::\n\n\n::: {.cell outputId='848af393-efb4-461c-8f72-ccbe71a39b78' execution_count=23}\n``` {.python .cell-code}\ntext_model.summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"model_1\"\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n_________________________________________________________________\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n Layer (type)                Output Shape              Param #   \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n=================================================================\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n text (InputLayer)           [(None, 1)]               0         \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                 \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n text_vectorization_1 (TextV  (None, 500)              0         \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n ectorization)                                                   \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                 \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n text_embedding (Embedding)  (None, 500, 3)            6000      \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                 \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n dropout_2 (Dropout)         (None, 500, 3)            0         \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                 \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n global_average_pooling1d_1   (None, 3)                0         \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n (GlobalAveragePooling1D)                                        \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                 \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n dropout_3 (Dropout)         (None, 3)                 0         \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                 \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n dense_1 (Dense)             (None, 32)                128       \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                 \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n fake (Dense)                (None, 1)                 33        \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                 \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n=================================================================\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nTotal params: 6,161\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nTrainable params: 6,161\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nNon-trainable params: 0\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n_________________________________________________________________\n```\n:::\n:::\n\n\n::: {.cell outputId='fd4e839d-d0cd-43d9-8143-3c5dc1c6289f' execution_count=24}\n``` {.python .cell-code}\nkeras.utils.plot_model(text_model) # model architecture\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nYou must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n```\n:::\n:::\n\n\n![Text Model Architecture](text_model.png)\n\n::: {.cell execution_count=25}\n``` {.python .cell-code}\ntext_model.compile(optimizer=\"adam\",\n              loss = keras.losses.BinaryCrossentropy(from_logits=True), # binary classification\n              metrics=[\"accuracy\"])\n```\n:::\n\n\n```python\n#| id: RAAe1xlykWPa\n#| colab: {base_uri: 'https://localhost:8080/'}\n#| outputId: 2ccc3840-14d4-4392-ff32-44b9ff3d767f\ntext_history = text_model.fit(train, \n                    validation_data=val,\n                    epochs = 50, \n                    verbose = False)\n```\n\n```python\n#| id: E9jzEJS1kZM3\n#| colab: {base_uri: 'https://localhost:8080/', height: 609}\n#| outputId: 7ed8b00f-7623-4c1f-86ec-1aa930be2ddd\nplot_val(text_history, 0.97, title = 'Model Performance with Text Input')\n```\n![Plot2](plot2.png)\n\nWe see a very similar trend with using article text only to classify an article as fake news or real news. The validation and training accuracy both reach over 97% accuracy over a number of epochs, and the validation accuracy is consistently better than the training accuracy, which showcases that there is no presence of overfitting.\n\n### 3. Both Article Title and Text\n\nNow, we can use both title and text and evaluate its performance. Since we wrote variables for both models individually, we can reuse this code and just change the end to combine both models into one. To do this, we just need to use a `layers.concatenate` layer to add these two inputs together.\n\n::: {.cell execution_count=26}\n``` {.python .cell-code}\n# title\ntitle_input = keras.Input(\n    shape=(1,),\n    name = \"title\", # same name as the dictionary key in the dataset\n    dtype = \"string\"\n)\n\n# text\ntext_input = keras.Input(\n    shape=(1,),\n    name = \"text\", # same name as the dictionary key in the dataset\n    dtype = \"string\"\n)\n\n# share an embedding layer with 16 elements\nshared_embedding = layers.Embedding(size_vocabulary, 16)\n```\n:::\n\n\n::: {.cell execution_count=27}\n``` {.python .cell-code}\ntitle_features = title_vectorize_layer(title_input) # apply this \"function TextVectorization layer\" to lyrics_input\ntitle_features = shared_embedding(title_features)\ntitle_features = layers.Dropout(0.2)(title_features)\ntitle_features = layers.GlobalAveragePooling1D()(title_features)\ntitle_features = layers.Dropout(0.2)(title_features)\ntitle_features = layers.Dense(32, activation='relu')(title_features)\n\ntext_features = text_vectorize_layer(text_input) # apply this \"function TextVectorization layer\" to lyrics_input\ntext_features = shared_embedding(text_features)\ntext_features = layers.Dropout(0.2)(text_features)\ntext_features = layers.GlobalAveragePooling1D()(text_features)\ntext_features = layers.Dropout(0.2)(text_features)\ntext_features = layers.Dense(32, activation='relu')(text_features)\n```\n:::\n\n\n::: {.cell execution_count=28}\n``` {.python .cell-code}\nmain = layers.concatenate([title_features, text_features], axis = 1)\n```\n:::\n\n\n::: {.cell execution_count=29}\n``` {.python .cell-code}\nmain = layers.Dense(32, activation='relu')(main)\noutput = layers.Dense(1, name=\"fake\")(main)\n```\n:::\n\n\n::: {.cell execution_count=30}\n``` {.python .cell-code}\n# title + text model\ntt_model = keras.Model(\n    inputs = [title_input, text_input],\n    outputs = output\n)\n```\n:::\n\n\n::: {.cell outputId='fdcb457a-dbb2-4560-fe82-a12d787784f8' execution_count=31}\n``` {.python .cell-code}\ntt_model.summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"model_2\"\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n__________________________________________________________________________________________________\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n Layer (type)                   Output Shape         Param #     Connected to                     \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n==================================================================================================\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n title (InputLayer)             [(None, 1)]          0           []                               \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                                                  \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n text (InputLayer)              [(None, 1)]          0           []                               \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                                                  \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n text_vectorization (TextVector  (None, 500)         0           ['title[0][0]']                  \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n ization)                                                                                         \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                                                  \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n text_vectorization_1 (TextVect  (None, 500)         0           ['text[0][0]']                   \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n orization)                                                                                       \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                                                  \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n embedding (Embedding)          (None, 500, 16)      32000       ['text_vectorization[1][0]',     \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                  'text_vectorization_1[1][0]']   \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                                                  \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n dropout_4 (Dropout)            (None, 500, 16)      0           ['embedding[0][0]']              \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                                                  \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n dropout_6 (Dropout)            (None, 500, 16)      0           ['embedding[1][0]']              \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                                                  \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n global_average_pooling1d_2 (Gl  (None, 16)          0           ['dropout_4[0][0]']              \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n obalAveragePooling1D)                                                                            \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                                                  \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n global_average_pooling1d_3 (Gl  (None, 16)          0           ['dropout_6[0][0]']              \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n obalAveragePooling1D)                                                                            \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                                                  \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n dropout_5 (Dropout)            (None, 16)           0           ['global_average_pooling1d_2[0][0\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                 ]']                              \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                                                  \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n dropout_7 (Dropout)            (None, 16)           0           ['global_average_pooling1d_3[0][0\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                 ]']                              \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                                                  \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n dense_2 (Dense)                (None, 32)           544         ['dropout_5[0][0]']              \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                                                  \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n dense_3 (Dense)                (None, 32)           544         ['dropout_7[0][0]']              \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                                                  \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n concatenate (Concatenate)      (None, 64)           0           ['dense_2[0][0]',                \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                  'dense_3[0][0]']                \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                                                  \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n dense_4 (Dense)                (None, 32)           2080        ['concatenate[0][0]']            \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                                                  \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n fake (Dense)                   (None, 1)            33          ['dense_4[0][0]']                \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                                                  \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n==================================================================================================\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nTotal params: 35,201\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nTrainable params: 35,201\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nNon-trainable params: 0\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n__________________________________________________________________________________________________\n```\n:::\n:::\n\n\n::: {.cell outputId='f2ac09a4-145c-4664-84a3-570784ba42fa' execution_count=32}\n``` {.python .cell-code}\nkeras.utils.plot_model(tt_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nYou must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n```\n:::\n:::\n\n\n![Title-Text Model Architecture](tt_model.png)\n\n::: {.cell execution_count=33}\n``` {.python .cell-code}\ntt_model.compile(optimizer=\"adam\",\n              loss = keras.losses.BinaryCrossentropy(from_logits=True), # binary classification\n              metrics=[\"accuracy\"])\n```\n:::\n\n\n```python\n#| id: 2Vy4ZiqMqEiW\ntt_history = tt_model.fit(train, \n                    validation_data=val,\n                    epochs = 50, \n                    verbose = False)\n```\n\n```python\n#| id: S35tT4KxqLaP\n#| colab: {base_uri: 'https://localhost:8080/', height: 609}\n#| outputId: 19dd5a3e-50fc-4914-89a5-48ab12b55382\nplot_val(tt_history, 0.97, title = 'Model Performance with Text Input')\n```\n![Plot3](plot3.png)\n\nThis model also performs insanely well, it seems that the training and validation accuracy both reach extremely closely to 100% accuracy. There is no overfitting in this case, but it does seem to be nearing a perfect classification.\n\n### Which one to use?\n\nAs you can see, all three of these models performed exceptionally well. Which one do we choose? In this case, I am going to be looking at the summary statistics for the validation accuracies for the last 30 epochs to see which one I would like to use.\n\n```python\n#| id: IcsbBz86srfj\n#| colab: {base_uri: 'https://localhost:8080/'}\n#| outputId: 217480cb-8920-496b-e2c0-b67b73ecc4a1\nprint(\"Title:\")\nprint(pd.Series(title_history.history['val_accuracy'][-30:]).describe())\nprint(\"\\nText:\")\nprint(pd.Series(text_history.history['val_accuracy'][-30:]).describe())\nprint(\"\\nTitleText:\")\nprint(pd.Series(tt_history.history['val_accuracy'][-30:]).describe())\n```\n![Summary Stats](summ_stats.png)\nWe can see that the mean and the median are both at their peak with the model containing both the title and text information. The miniumum accuracy of the last model does not even go below 99.9% accuracy, which the other two models cannot even hit. Therefore, we will use the last model on the unseen test data in hopes of good performance.\n\n## Model Evaluation\n\nLet us look at the performance on the unseen test data. The test data is downloaded as follows:\n\n::: {.cell execution_count=34}\n``` {.python .cell-code}\n# load test data\ntest_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true\"\ntest_news = pd.read_csv(test_url)\ntest_news.columns = ['id', 'title', 'text', 'fake']\n```\n:::\n\n\n::: {.cell execution_count=35}\n``` {.python .cell-code}\ntest_data = make_dataset(test_news, ['title', 'text'], 'fake')\n```\n:::\n\n\n```python\n#| id: 2b0JzXvTv5dV\n#| colab: {base_uri: 'https://localhost:8080/'}\n#| outputId: c3b7fdd0-aeeb-496b-ad56-1af31d465029\ntest_loss, test_acc = tt_model.evaluate(test_data)\n\nprint('Test Loss: ', test_loss)\nprint('Test Accuracy: ', test_acc)\n```\n![Test Results](test_acc.png)\n\nAccording to the test data, we are able to gain above 97% accuracy, which is amazingly good. It would be right about 97% of the time, which is helpful especially when it comes to misinformation.\n\n## Embedding Visualization\n\nNow, we will look a bit deeper into the embedding layer for this model.\n\n```python\n#| id: HUShOoyf440V\nvocab = text_vectorize_layer.get_vocabulary() # keeps track of mapping from word to integer\n```\n\n```python\n#| id: V_1Y1y-Z5Qxz\ntext_weights = tt_model.get_layer(\"embedding\").get_weights()[0] # weights for embedding each of the 2000 words\n```\n\n```python\n#| id: SHrs-Yxo5Z-5\n#| colab: {base_uri: 'https://localhost:8080/'}\n#| outputId: d749721a-93b6-4901-b553-feca3508c4ca\ntext_weights.shape\n```\n\n```python\n#| id: i_2UBndX5XZN\nfrom sklearn.decomposition import PCA \n# https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n# principal components analysis - \n# project things to lower dimension such that the variance of the dataset is most preserved\n\npca = PCA(n_components=2)\nweights = pca.fit_transform(text_weights)\n```\n\n```python\n#| id: SoQgrX1D5fhX\nembedding_df = pd.DataFrame({\n    'word': vocab,\n    'x0': text_weights[:, 0],\n    'x1': text_weights[:, 1]\n}) # on PCA axes\n```\n\n```python\n#| id: pfmVqcYR5kxq\n#| colab: {base_uri: 'https://localhost:8080/', height: 542}\n#| outputId: f1271819-bae4-48db-d6e8-06364184bc90\nimport plotly.express as px\nfig = px.scatter(embedding_df,\n                 x='x0',\n                 y='x1',\n                 size=[2]*len(embedding_df),\n                 #size_max = 2,\n                 hover_name = 'word' \n                 )\nfig.show()\n```\n\n![](plotlysad.png)\n\nFrom this visualization, we can see that the embedding goes to two different directions on the two major principal component axes. On the bottom left, we have words such as `\"gop\"`, `\"rep\"`, `\"reportedly\"`. These touch upon politics (i.e. elections), but what ties these words together is this word `\"21wiretv\"`. After doing some research, it is found that `\"21wiretv\"` is a journalism website that has a reputation of conspiracy theories and/or hoaxes. This leads me to believe that this side errs on **fake news**. On the other hand, we have terms such as `\"trumps\"`, `\"obamas\"`, `\"partys\"`. It talks about politics, but particularly about presidents. We also see words like `\"opinions\"`, which leads me to believe that this end is more about **real news** because they are willing to state opinion as opposed to deceive people by saying it is fact.\n\n## Conclusion\n\nFrom this article, we have learned to process a dataset with text inputs so that it is fit for training through neural networks. We then learned how to use the functional API to create three different models. Then, we sought to interpret the embedding to see how the model learned how to classify fake news. This most definitely can be used for many other applications that use text, so go out and explore this world of working with text data!\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}