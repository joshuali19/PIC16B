{
  "hash": "8f6284e92b44403bb1be0f2549ee8220",
  "result": {
    "markdown": "---\ntitle: Classifying Pets through Convolutional Neural Networks with Tensorflow\nauthor: Joshua Li\ndate: '2023-02-27'\nsidebar:\n  style: docked\n  search: true\n  contents: auto\n---\n\n![The CNN classifies this as a good doggo](doggo.png)\n\nFor this blog post, we will be looking into image classification of pets using Python's Tensorflow package to create a convolutional neural network (CNN). In this post, you will learn:\n\n1. [How to obtain image data](#imagedata)\n2. [How to create a basic CNN](#model1)\n3. [How to augment your data](#data-augmentation)\n4. [How to preprocess your data](#data-preprocessing)\n5. [How to perform transfer learning](#transfer-learning)\n6. [How to evaluate your model](#score-on-test-data)\n\nWe will be working with an image dataset that works with images of cats and dogs. The CNN will take these input images and train on these images to get a model that would predict and classify any new images of cats or dogs. \n\n# [Getting Image Data]{#imagedata}\n\nThe first thing we need to do is to have data to work with. Please run the following code in order to load the necessary libraries for this tutorial, as well as the necessary image files to work on.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport os\nfrom tensorflow.keras import utils, layers\nimport tensorflow as tf\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport random\ntf.get_logger().setLevel('ERROR')\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n2023-02-28 10:32:40.284154: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n```\n:::\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# location of data\n_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'\n\n# download the data and extract it\npath_to_zip = utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)\n\n# construct paths\nPATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')\n\ntrain_dir = os.path.join(PATH, 'train')\nvalidation_dir = os.path.join(PATH, 'validation')\n\n# parameters for datasets\nBATCH_SIZE = 32\nIMG_SIZE = (160, 160)\n\n# construct train and validation datasets \ntrain_dataset = utils.image_dataset_from_directory(train_dir,\n                                                   shuffle=True,\n                                                   batch_size=BATCH_SIZE,\n                                                   image_size=IMG_SIZE)\nclass_names = train_dataset.class_names\nvalidation_dataset = utils.image_dataset_from_directory(validation_dir,\n                                                        shuffle=True,\n                                                        batch_size=BATCH_SIZE,\n                                                        image_size=IMG_SIZE)\n\n# construct the test dataset by taking every 5th observation out of the validation dataset\nval_batches = tf.data.experimental.cardinality(validation_dataset)\ntest_dataset = validation_dataset.take(val_batches // 5)\nvalidation_dataset = validation_dataset.skip(val_batches // 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFound 2000 files belonging to 2 classes.\nFound 1000 files belonging to 2 classes.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n2023-02-28 10:32:53.674734: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n```\n:::\n:::\n\n\nThis following chunk of code will configure the dataset for performance. Specifically the `prefetch()` function will overlap data preprocessing and model execution while training. If you would like to learn more about how this works, please check out [this article](https://www.tensorflow.org/guide/data_performance).\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nAUTOTUNE = tf.data.AUTOTUNE\n\ntrain_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\nvalidation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)\ntest_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)\n```\n:::\n\n\nNow that we have our data loaded, let's look at some of the images in this dataset.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ndef catdog(df):\n    plt.figure(figsize=(10, 10))\n\n    # for images in a batch\n    for images, labels in df.take(1):\n        # get 3 random labels of cats and dogs\n        cat_lbls = random.sample(set(np.where(labels == 0)[0]), 3)\n        dog_lbls = random.sample(set(np.where(labels == 1)[0]), 3)\n        lbls = cat_lbls + dog_lbls # merge lists\n        # display images row-wise\n        for idx, lbl_num in enumerate(lbls):\n            ax = plt.subplot(2, 3, idx + 1)\n            plt.imshow(images[lbl_num].numpy().astype(\"uint8\"))\n            plt.title(class_names[labels[lbl_num]])\n            plt.axis(\"off\")\n```\n:::\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\ncatdog(train_dataset)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-1.png){width=763 height=661}\n:::\n:::\n\n\nFrom this output, we can see a sample of some of the images of the pets. These come with a variety of breeds, in different positions, and could contain multiple of one pet. This gives us a good sense of the data we are working with. \n\nNow, lets look at the benchmark accuracy for the training dataset.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nlabels_iterator= train_dataset.unbatch().map(lambda image, label: label).as_numpy_iterator()\n```\n:::\n\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nlength = 0\ndogs = 0\nfor lbl in labels_iterator:\n    length += 1\n    if lbl == 1:\n        dogs += 1\ncats = length - dogs\n{'dogs': dogs, 'cats': cats} # get the number of dogs and cats\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\n{'dogs': 1000, 'cats': 1000}\n```\n:::\n:::\n\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\ndogs / (dogs + cats)\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\n0.5\n```\n:::\n:::\n\n\nThe benchmark accuracy for the model should be 50%.\n\n## [Model 1]{#model1}\n\nWith the benchmark in mind, let us start by making our first model. We will first start off with 3 Convolution layers and MaxPooling layers, adding a dropout layer in hopes to prevent overfitting, and flatten out the nodes and to classify the input as either a cat or a dog. Read more about the different types of layers for a convolutional neural network (CNN) [here](https://www.tensorflow.org/api_docs/python/tf/keras/layers). We also need to `compile()` the model, which will determine what metrics and optimizer we use to evaluate the performance of the model on the training and validation data. Based on literature and recommendations, this model will use the Adam optimizer for stochastic gradient descent, and evaluate the performance through binary cross-entropy and accuracy.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nmodel1 = tf.keras.Sequential([\n    layers.Conv2D(16, (3, 3), activation='relu', input_shape = (160, 160, 3)),\n    layers.MaxPooling2D(),\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D(),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D(),\n    layers.Dropout(0.05),\n    layers.Flatten(),\n    layers.Dense(32, activation = 'relu'),\n    layers.Dense(1, activation = 'sigmoid')\n])\n\nmodel1.compile(optimizer = 'adam', loss = tf.keras.losses.BinaryCrossentropy(),\n              metrics = ['accuracy'])\n```\n:::\n\n\n```python\nmodel1.summary()\n```\n![Model 1 Summary](model1_summary.png)\n\n```python\nhistory = model1.fit(train_dataset, \n                     epochs=20, \n                     validation_data=validation_dataset)\n```\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\ndef acc_eval(history, title, threshold):\n    '''\n    creates a plot of accuracy over epoches.\n    @ input:\n    - history: the neural network\n    - title (str): Title of Visualization\n    @ output:\n    - plot: plot of accuracy\n    '''\n    plt.plot(history.history['accuracy'], label = 'training')\n    plt.plot(history.history['val_accuracy'], label = 'validation')\n    plt.legend()\n    plt.title(title)\n    plt.xticks(np.arange(0, 21, 2))\n    plt.xlabel(\"epoch\")\n    plt.ylabel(\"accuracy\")\n    plt.axhline(y = threshold, color = 'r', linestyle = 'dashed')\n    plt.show()\n```\n:::\n\n\n```python\nacc_eval(history, \"Model 1\", 0.52)\n```\n![Model 1 Performance](model1.png)\n\n**As seen from the output of the model fit and the visualization, we can see that the output stablized between 55% and 60% validation accuracy during training.** This is only slightly better than the baseline accuracy of 50%. When evaluating for overfitting, we need to look at the difference between the training accuracy and the validation accuracy. Since the training accuracy has trained to be more than 90% accurate while the validation accuracy stayed below 60%, it is reasonable to say that the model is overfitting to the training data.\n\n# Data Augmentation\n\nNow, we are going to make some changes to our model to see if we can solve this problem of overfitting and gain a better generalized model. One of the steps is to manipulate and augment the data to prevent the model from fixating on fixed position.\n\n### Flipping the Images\n\nThe first way we will augment the images is to flip the image through the `tf.keras.layers.RandomFlip()` layer. By default, the layer would randomly flip the image horizontally or vertically.\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nflip = tf.keras.Sequential([\n  layers.RandomFlip(\"horizontal_and_vertical\")\n])\n```\n:::\n\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nplt.figure(figsize=(10, 10))\nfor images, labels in train_dataset.take(1):\n    for i in range(9):\n        if i != 0:\n            augmented_image = flip(images, training = True)\n        else:\n            augmented_image = images\n        ax = plt.subplot(3, 3, i + 1)\n        plt.imshow(augmented_image[0].numpy().astype('uint8'))\n        plt.axis(\"off\")\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-13-output-1.png){width=762 height=758}\n:::\n:::\n\n\nThis shows the original image in the top-left image, and then shows how the `RandomFlip()` function works. It will either invert the image horizontally, vertically, or both ways.\n\n### Rotating the Image\n\nThe second way we will transform our image is to rotate it, which is through the `tf.keras.layers.RandomRotation()` layer. We need to specify a factor of `2*pi` so that there is a bound to the rotation.\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\nrotate = tf.keras.Sequential([\n  layers.RandomRotation(0.2)\n])\n```\n:::\n\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nplt.figure(figsize=(10, 10))\nfor images, labels in train_dataset.take(1):\n    for i in range(9):\n        if i != 0:\n            augmented_image = rotate(images, training = True)\n        else:\n            augmented_image = images\n        ax = plt.subplot(3, 3, i + 1)\n        plt.imshow(augmented_image[0].numpy().astype('uint8'))\n        plt.axis(\"off\")\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-15-output-1.png){width=762 height=758}\n:::\n:::\n\n\nWith this function, it will randomly rotate the original image (top-left) and fill in any extraneous space with black pixels. You can visibly see that the image will take on different angles as it is called mutiple times.\n\nNow, with these two functions in mind, we can create a model that incorporates this data augmentation.\n\n## [Model 2]{#model2}\n\nThis is the second model. It will contain the same architecture as that of [Model 1](#model1) and add on the functions that we have just created.\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\nmodel2 = tf.keras.Sequential([\n    flip,\n    rotate,\n    layers.Conv2D(16, (3, 3), activation='relu', input_shape = (160, 160, 3)),\n    layers.MaxPooling2D(),\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D(),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D(),\n    layers.Dropout(0.01),\n    layers.Flatten(),\n    layers.Dense(32, activation = 'relu'),\n    layers.Dense(1, activation = 'sigmoid')\n])\n\nmodel2.compile(optimizer = 'adam', loss = tf.keras.losses.BinaryCrossentropy(),\n              metrics = ['accuracy'])\n```\n:::\n\n\n```python\nhistory = model2.fit(train_dataset, \n                     epochs=20, \n                     validation_data=validation_dataset)\n```\n\n```python\nmodel2.summary()\n```\n![Model 2 Summary](model2_summary.png)\n\n```python\nacc_eval(history, \"Model 2\", 0.55)\n```\n![Model 2 Performance](model2.png)\n\nWe can see that the **validation accuracy consistently falls between 56% and 62%**. This is a slight improvement compared to the first model. In this case, we still see slight overfitting, in that the training accuracy is still somewhat higher than the validation accuracy, although the difference is not as drastic compared to the first model. The training accuracy improved much slower in this model compared to the first model. Let's see how we can further improve this model\n\n# Data Preprocessing\n\nOne of the ways is through data preprocessing, where you transform the data to become normalized. The data currently has RGB values that range between 0 and 255, but we can normalize this to be between 0 and 1 or -1 and 1. In order to do this, we need to create a preprocessing layer called preprocessor which we can add to our model pipeline.\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\ni = tf.keras.Input(shape=(160, 160, 3))\nx = tf.keras.applications.mobilenet_v2.preprocess_input(i)\npreprocessor = tf.keras.Model(inputs = [i], outputs = [x])\n```\n:::\n\n\nWith this code, we can now add the `preprocessor` layer to the beginning of our model pipeline, even before our data augmentation.\n\n## [Model 3]{#model3}\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\nmodel3 = tf.keras.Sequential([\n    preprocessor,\n    flip,\n    rotate,\n    layers.Conv2D(16, (3, 3), activation='relu', input_shape = (160, 160, 3)),\n    layers.MaxPooling2D(),\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D(),\n    layers.Dropout(0.2),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D(),\n    layers.Dropout(0.3),\n    layers.Flatten(),\n    layers.Dense(32, activation = 'relu'),\n    layers.Dense(1, activation = 'sigmoid')\n])\n\nmodel3.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.0006), loss = tf.keras.losses.BinaryCrossentropy(),\n              metrics = ['accuracy'])\n```\n:::\n\n\n```python\nhistory = model3.fit(train_dataset, \n                     epochs=20, \n                     validation_data=validation_dataset)\n```\n\n```python\nmodel3.summary()\n```\n![Model 3 Summary](model3_summary.png)\n\n```python\nacc_eval(history, \"Model 3\", 0.70)\n```\n![Model 3 Performance](model3.png)\n\nFrom this model, we can see that the preprocessing has helped a lot. **The validation accuracy fluctuates from 70% to 73% as the model trains through more epochs**. This performs well compared to the baseline accuracy of 50%, as well as the accuracy of 52% for the first model. In order to prevent overfitting, we increased the percentage of observations to drop out in the `dropout()` layer. In this case, overfitting is not an issue in comparison to the first two models, as the difference between the training and validation accuracy is not as drastic.\n\n# Transfer Learning\n\nFrom the beginning of the blog post, we have created our own neural network architecture. However, there may be cases where people have built pre-existing models that may have benefits for the task at hand. Let us first instantiate the pre-existing base model.\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\nIMG_SHAPE = IMG_SIZE + (3,)\nbase_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n                                               include_top=False,\n                                               weights='imagenet')\nbase_model.trainable = False\n\ni = tf.keras.Input(shape=IMG_SHAPE)\nx = base_model(i, training = False)\nbase_model_layer = tf.keras.Model(inputs = [i], outputs = [x])\n```\n:::\n\n\nWith the `base_model_layer`, we can create a model that utilizes this layer to create a model with this underlying framework.\n\n## [Model 4]{#model4}\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\nmodel4 = tf.keras.Sequential([\n    preprocessor,\n    flip,\n    rotate,\n    base_model_layer,\n    layers.GlobalMaxPooling2D(),\n    layers.Dropout(0.2),\n    layers.Dense(1, activation = 'sigmoid')\n])\n\nmodel4.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.0006), loss = tf.keras.losses.BinaryCrossentropy(),\n              metrics = ['accuracy'])\n```\n:::\n\n\n```python\nmodel4.summary()\n```\n![Model 4 Summary](model4_summary.png)\n\n```python\nhistory = model4.fit(train_dataset, \n                     epochs=20, \n                     validation_data=validation_dataset)\n```\n\n```python\nacc_eval(history, \"Model 4\", 0.95)\n```\n![Model 4 Performance](model4.png)\n\nWith this model, we can see that the **validation accuracy reaches above the desired threshold of 95% after the second epoch**. This validation accuracy is substantially higher than the first three models that we made. This is also the first where the validation accuracy exceeds that of the corresponding training accuracy. In this case, there is no overfitting, as the training accuracy does not differ from the validation accuracy greatly.\n\n# Score on Test Data\n\nNow, we have to retrieve the classification of the images in the test dataset. This will inform us of our model's performance on unseen data to see if it truly is a generalizable model.\n\n```python\ntest_loss, test_acc = model4.evaluate(test_dataset)\n```\n![Test Accuracy](test_acc.png)\n\nWe can see that our testing accuracy is 95.83%, which is consistent with the findings from the validation accuracy for [model 4](#model4).\n\n# Conclusion\n\nWith this post, we have learned how to create a neural network, and to add different types of layers to better the performance! It's crazy to see how accurate some of these models can get. It makes some of the coding scenes in movies seem just a little more realistic. Play around with these layers for yourself and try to get it to be as accurate as possible.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}